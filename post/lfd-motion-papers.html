<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【模仿动作】从人类演示中学习机器人动作规划方法 | 马浩飞丨博客</title><meta name="keywords" content="模仿,视觉,深度学习,机器人动作"><meta name="author" content="马浩飞"><meta name="copyright" content="马浩飞"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="从人类抓取物体的视频中学习机器人的动作生成与规划方法，相关的研究的调研。">
<meta property="og:type" content="article">
<meta property="og:title" content="【模仿动作】从人类演示中学习机器人动作规划方法">
<meta property="og:url" content="https://www.mahaofei.com/post/lfd-motion-papers.html">
<meta property="og:site_name" content="马浩飞丨博客">
<meta property="og:description" content="从人类抓取物体的视频中学习机器人的动作生成与规划方法，相关的研究的调研。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.mahaofei.com/img/202309071622980.png">
<meta property="article:published_time" content="2023-09-07T06:41:50.000Z">
<meta property="article:modified_time" content="2023-09-07T06:41:50.000Z">
<meta property="article:author" content="马浩飞">
<meta property="article:tag" content="模仿">
<meta property="article:tag" content="视觉">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="机器人动作">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.mahaofei.com/img/202309071622980.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.mahaofei.com/post/lfd-motion-papers"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-bB89NudWgv"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?439a0d0abeb31dd8f338efd8266c999b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ABY1KMOQQM","apiKey":"d3f3a4fbb355106e6bf265cf8da1863b","indexName":"hexo","hits":{"per_page":4},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【模仿动作】从人类演示中学习机器人动作规划方法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-07 14:41:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://unpkg.com/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="/css/custom/MainColor.css"><link rel="stylesheet" href="/css/custom/categoryBar.css"><link rel="stylesheet" href="/css/custom/404.css"><link rel="stylesheet" href="/css/custom/cardHistory.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><link rel="stylesheet" href="/css/custom/custom.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="马浩飞丨博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.mahaofei.com/img/202309071622980.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">马浩飞丨博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【模仿动作】从人类演示中学习机器人动作规划方法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-07T06:41:50.000Z" title="发表于 2023-09-07 14:41:50">2023-09-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-07T06:41:50.000Z" title="更新于 2023-09-07 14:41:50">2023-09-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/">机器人</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E4%B9%A0/">机器人学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>32分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【模仿动作】从人类演示中学习机器人动作规划方法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/lfd-motion-papers.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>1 MimicPlay: Long-Horizon Imitation Learning by Watching Human Play</h1>
<blockquote>
<p><strong>标题</strong>：模拟游戏：通过观看人类游戏进行的长期模拟学习<br>
<strong>作者团队</strong>：斯坦福大学<br>
<strong>期刊会议</strong>：CoRL<br>
<strong>时间</strong>：2023<br>
<strong>代码</strong>：<a target="_blank" rel="noopener" href="https://mimic-play.github.io/">https://mimic-play.github.io/</a>(code is coming soon)</p>
</blockquote>
<h2 id="1-1-目标问题-4">1.1 目标问题</h2>
<p>由于人类比遥控机器人能更快的完成长时间任务，因此启发从人类演示中学习机器人规划策略。</p>
<p>为了减少数据需求，采用人类与环境的交互视频作为数据。开发一个分层学习框架，从人类数据中学习潜在的规划控制方法。</p>
<h2 id="1-2-方法-4">1.2 方法</h2>
<p><img src="https://img.mahaofei.com/img/202309071622980.png" alt="image.png"></p>
<p><strong>（1）从人类数据中学习潜在规划</strong></p>
<p>给定输入：视觉观察$o_t$，未来的目标图像$g_t$，当前手部位置$l_t$<br>
训练过程中，$g_t$被视为执行动作后的未来帧<br>
规划期的目标是根据视频提示V生成目标图像的动作规划。</p>
<ol>
<li>人类演示数据收集</li>
<li>跟踪人手三维轨迹：使用双目相机获取人手的3D轨迹，利用现成的<a target="_blank" rel="noopener" href="https://github.com/ddshan/hand_object_detector">手部检测器</a>确定2维图像中的手部位置，然后利用双目视图重建手的3D轨迹。</li>
<li>学习潜在规划：使用两个卷积网络分别将当前图像和目标图像处理为低维特征，再与手部位置连接在一起，使用MLP处理为潜在规划特征。生成3D手部运动轨迹。为了处理同一个任务的不同方式的实现，使用高斯混合模型对潜在规划的轨迹分布进行建模。</li>
</ol>
<p><strong>（2）计划引导的多任务模仿学习</strong></p>
<p>机器人的底层策略使用行为克隆算法进行训练，使用通过遥操作收集的机器人演示数据。</p>
<ol>
<li>视频条件下的潜在规划生成：使用遥操作机器人任务视频来提示训练时潜在规划器生成相应的规划。</li>
<li>基于Transformer的规划引导模仿：将机器人手上相机观察和本体姿态信息处理为低维向量，再与潜在计划连接起来，通过Transformer架构来计算最终的机器人控制命令。</li>
<li>多任务学习</li>
</ol>
<h2 id="1-3-思考-4">1.3 思考</h2>
<p>李飞飞团队的作品，从视频中学习人手的运动轨迹，code is coming soon，等待后续再细看。</p>
<h1>2 One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</h1>
<blockquote>
<p><strong>标题</strong>：通过领域自适应元学习观察人类的一次性模仿<br>
<strong>作者团队</strong>：加州大学伯克利分校<br>
<strong>期刊会议</strong>：arXiv<br>
<strong>时间</strong>：2018<br>
<strong>代码</strong>：<br>
<a target="_blank" rel="noopener" href="https://github.com/tianheyu927/mil">官方版: https://github.com/tianheyu927/mil</a><br>
<a target="_blank" rel="noopener" href="https://github.com/daiyk/daml_pytorch">Pytorch版: https://github.com/daiyk/daml_pytorch</a></p>
</blockquote>
<h2 id="2-1-目标问题-2">2.1 目标问题</h2>
<p>提出一种从人类视频中进行学习的方法，通过使用各种先前任务的人类和机器人演示数据，使机器人执行人类演示的任务。</p>
<h2 id="2-2-方法-2">2.2 方法</h2>
<p><strong>（1）问题描述</strong></p>
<p>将先验知识和少量证据组合起来，形成一个人类演示的形式。从中推断出完成任务的机器人的策略参数。</p>
<p><strong>（2）领域自适应元学习</strong></p>
<p>能够处理从人类的视频演示中学习，学习一组参数，以便在人类演示的基础上进行梯度下降后，模型可以有效地执行新任务。</p>
<p>由于人类和机器人的动作无法直接对应，因此考虑学习只对策略激活起作用。</p>
<p><strong>（3）学习时间适应目标</strong></p>
<p>要从人类的视频中进行学习，需要捕获视频中的相关信息，例如人类的意图和任务的相关对象。要确定哪些行为正在被演示，哪些对象是相关的，通常需要同时检查多个帧来确定人类的运动。因此本文的学习适应目标将多个时间步长耦合，从多个时间步骤对策略进行操作。</p>
<p>此处使用卷积网络来表示自适应目标，使用递归神经网络LSTM进行时间卷积。</p>
<p><strong>（4）概率解释</strong></p>
<p>将学习到的自适应目标纳入到概率图模型的框架中，推断特定任务的策略参数。</p>
<h2 id="2-3-思考">2.3 思考</h2>
<p>思路看起来很可以，就是数学推理比较复杂，很难看得懂。</p>
<h1>3 Waypoint-Based Imitation Learning for Robotic Manipulation</h1>
<blockquote>
<p><strong>标题</strong>：基于航路点的机器人操纵模拟学习<br>
<strong>作者团队</strong>：斯坦福大学<br>
<strong>期刊会议</strong>：arXiv<br>
<strong>时间</strong>：2023<br>
<strong>代码</strong>：<a target="_blank" rel="noopener" href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></p>
</blockquote>
<p>行为克隆BC目前有很多问题，路径点可以通过减少BC的范围来解决这个问题，但是传统路径点需要人工监督标注。</p>
<p>本文提出了线性运动近似的，模仿学习的自动轨迹点提取模块，将演示分解为一组轨迹点，进行线性插值，近似实现演示动作。</p>
<p>并且该方法可以与任务BC算法相结合，提高其成功率。</p>
<h1>4 Building Robot Intelligence by Scaling Human Supervision</h1>
<blockquote>
<p><strong>标题</strong>：通过扩展人类监督构建机器人智能<br>
<strong>作者团队</strong>：Stanford University<br>
<strong>期刊会议</strong>：Thesis<br>
<strong>时间</strong>：2021</p>
</blockquote>
<h2 id="4-1-研究背景">4.1 研究背景</h2>
<p><strong>几十年来，我们一直在想象一个机器人可以充当个人助理的世界，能够完成我们每天做的各种任务和家务，比如做饭、打扫卫生、洗衣，甚至组装橱柜。机器人领域的研究人员一直致力于实现这一梦想。然而，不幸的是，今天的自主机器人远未达到操纵能力的水平。尽管研究在使机器人能够完全自主地完成特定任务的方面取得了令人印象深刻的进展，包括拾取物体，或将它们堆叠在一起。但机器人和人类的操作能力之间存在很大差距。人类智能地使用物体，并在日常生活中以丰富的方式与它们互动，比如当我们用刀切菜做饭时，或者用螺丝刀拧紧螺丝组装橱柜时。这种有目的的与物体的互动对机器人来说是十分困难的。</strong></p>
<p><strong>作为人类，我们在一生中积累了一系列不同的先前经验，这些经验我们可以在日常生活中借鉴。此外，即使我们不知道如何做某事，我们也可以通过观看其他人的视频来快速学习，例如通过观看YouTube上其他人组装橱柜的视频来学习如何组装橱柜。这就提出了一个问题——我们是否可以类似地为机器人提供丰富多样的先前经验，并使他们能够从这些数据集中学习操作技能？</strong></p>
<p><strong>这激发了数据驱动的机器人，这是一种有用的范式，让机器人从大型数据集中学习操作。但是这种方法通常有两种变体，第一种是机器人自行收集数据，数据一开始是随机的，但会随着时间推移慢慢变好。由于机器人必须自己学习，限制了可以学习的人物的复杂性。第二种则是人类控制机器人并引导它完成任务，然是这通常是不可扩展的，因此可以收集的数据量很小，这再次限制了任务的复杂性。</strong></p>
<p><strong>相比之下，计算机数据和自然语言处理等领域已经通过大规模高质量数据集开创了前所未有的成就，我们希望在机器人技术方面看到类似的突破。</strong></p>
<p><strong>为了复制这一成功经验，并解决数据驱动机器人中任务复杂性有限的问题，我们需要解决两个关键挑战。首先，收集大规模的人类数据具有挑战性。在计算机视觉领域，注释可以由人类直接标注，很容易实现并行标注和大规模人员标注。相比之下，在机器人技术中，人类必须与机器人实时互动，引导机器人完成任务。这使得提供直观和可扩展的方法来收集来自多个人的数据变得很困难。其次，从大规模数据集中学习可能并不简单。在其他领域，我们可以训练网络预测注释，这些注释对应的都是真实的标签。然而在机器人技术中，没有一种真正的方法来执行任务，不同的人可能会收集不同的轨迹，不同的策略，我们需要确定如何从这些数据集中学习。</strong></p>
<h2 id="4-2-研究目标">4.2 研究目标</h2>
<p>第一部分，讨论了如何通过充满丰富交互的人类监督来收集大规模数据，这些数据体现了机器人的类人操作能力。包括一个为解决机器人操作中对大规模人类数据集需求构建的平台，和现实世界的数据收集。</p>
<p>第二部分，讨论了如何使用丰富的数据集来学习机器人操作技能。</p>
<p>第三部分，讨论了该方法可能的进一步拓展和应用。</p>
<h2 id="4-3-收集人类操作数据">4.3 收集人类操作数据</h2>
<p><strong>为了使数据能够捕捉人类的操作，首先数据应该在所展示的解决问题的策略的种类上是多样化的。作为人类，我们很清楚什么时候应该尝试不同的方法类实现目标，而机器人应该从所有这些策略中学习，因为在特定的情况下可能需要其中的一种。其次，数据应该包含灵巧的操作，我们希望我们的机器人了解它们如何通过武力方式操作物体来实现预期的结果。最后，数据应该是大规模的，人类非常擅长在无数情况下解决问题，但机器人还不能做到这一点。我们向他们展示的数据越多，他们也就越有可能获得这种能力。</strong></p>
<p>在这一部分，我们提出了RoboTurk平台，一个数据收集平台，允许人类实时远程操作机器人。操作员在他们的网络浏览器中看到机器人的工作空间的视频流，用他们的智能手机控制机械臂，他们手机的运动与机器人的运动相耦合，可以自然地控制手臂，这使得人们可以轻松的提供任务演示，连接的过程快速而简单。实验表明，这些数据能够在多步骤操作任务上进行策略学习，并且在策略学习的过程中使用大量的演示可以在学习一致性和最终性能方面带来好处。</p>
<h2 id="4-4-从大规模人类数据集中学习操作">4.4 从大规模人类数据集中学习操作</h2>
<p><strong>在这一部分，我们讨论了机器人如何才能够大规模人类数据集中学习操作技能。此类数据集可能表现出巨大的多样性，并由次优解决方案组成，因此从中学习具有挑战性。我们提出了一种从大规模演示数据集中学习的新算法，即无规模交互的内隐强化IRIS算法。IRIS将控制问题分解为目标条件的低级控制器和高级目标选择机制，前者模仿短演示序列，后者为低级控制器设置目标，并选择性的组合部分次优解决方案，从而更成功的完成任务。</strong></p>
<p>尽管最近在模仿学习和强化学习方面缺乏开源的人类数据集和可重复的学习方法，使得评估该领域的状态变得困难。我们对六个离线学习机器人操作算法，在五个仿真和三个不同复杂度的真实环境中进行多阶段操作任务测试。我们得到了一系列经验，包括对不同算法设计选择的敏感性，对演示质量的依赖性，以及由于训练不同的目标而导致的不同停止标准。我们还强调了从人类数据集学习的可能性，例如在当前强化学习方法范围之外的具有挑战性的多阶段任务中学习熟练策略的能力，以及轻松扩展到只有原始感官信号可用的自然、真实世界操作场景的能力。我们已经开源了我们的数据集和所有算法实现，以促进未来的研究和从人类演示数据中学习的公平比较。</p>
<h2 id="4-5-使用人类数据集构建能力更强的机器人">4.5 使用人类数据集构建能力更强的机器人</h2>
<p>这一部分探讨了几个不同的应用程序，使我们更接近于我们希望的机器人能够在未来能够处理的任务。主要探讨了多任务领域（如厨房），高精度操作，和需要协作的多臂操作任务。</p>
<p><strong>模仿学习方法的一个常见的局限是由于训练集中的数据有限，在所展示的行为之外进行泛化是一个开放的挑战。例如在厨房场景中，我们可能希望机器人实现多种可能的配置，具有多个要操作和交互的对象，如食物、出轨、微波炉、水槽等。本章我们介绍了通过模仿进行任务泛化，这是一种新颖的模仿学习框架，使机器人能够从少量的人类演示中有效的学习复杂的现实世界操作任务。合成收集的演示中未包含的新行为。多任务领域通常呈现出一种潜在的结构，不同的任务轨迹在状态空间的公共区域相交。GTI是一个两阶段在线模仿学习算法，该算法利用交叉结构来训练目标导向的策略，这些测类推广到看不见的开始和目标状态组合。在GTI的第一阶段，我们训练了一个随机策略，该策略利用轨迹交叉点来有能力从不同的演示轨迹中组合行为在一起。在GTI的第二阶段，我们从第一阶段的无条件随机策略中收集了一小组推理，并训练一个目标导向的agent来推广到新的启动和目标配置。我们在模拟领域和现实世界中具有挑战性的长期机器人操作领域中验证了GTI。</strong></p>
<p><strong>模仿学习方法通常也很难完成高精度的操作任务，因为它们需要一系列精确的动作才能取得有意义的进展，比如机器人将pod插入咖啡机制作咖啡。经过培训的策略可能会在这些场景失败，因为行动上的微小偏差可能会导致策略进入未被演示覆盖的区域。基于干预的策略学习是解决这一问题的一种替代方案——它允许操作员监控经过训练的策略，并在遇到故障时接管控制权。</strong> 我们扩展了RoboTurk，使远程操作员能够监控和干预经过培训的政策。我们开发了一个简单的在系统收集的新数据上迭代训练策略的有效算法。我们证明，根据我们基于干预的系统和算法收集的数据训练的代理优于根据非干预演示者收集的同等数量样本训练的代理，并进一步证明，我们的方法在从具有挑战性的机器人线程任务和咖啡制作任务。</p>
<p><strong>最后，虽然通过远程操作收集的人类演示中的模仿学习（IL）是教授机器人操作技能的强大范式，但它大多局限于单臂操作。然而，许多现实世界中的任务需要多个手臂，例如举起重物或组装桌子。不幸的是，将IL应用于多臂操作任务一直具有挑战性</strong>——要求人类控制多个机械臂可能会带来巨大的认知负担，而且通常最多只能控制两个机械臂。为了应对这些挑战，我们介绍了多臂RoboTurk（MART），这是一个多用户数据收集平台，允许多个远程用户同时远程操作一组机械臂，并收集多臂任务的演示。使用MART，我们从几个地理位置不同的用户那里收集了五个新的双臂和三臂任务的演示。我们表明，从这些数据中学习因此给集中式代理带来了挑战，这些代理直接尝试同时对所有机器人动作进行建模，并对数据进行全面不同的策略架构，对我们的任务具有不同的集中程度。最后，<strong>我们提出并评估了一个基本残差策略框架，该框架允许经过训练的策略更好地适应多臂操作中常见的混合协调设置，并表明用去中心化残差模型增强的集中式策略在我们的基准任务集上优于所有其他模型。</strong></p>
<h1>5 Understanding and Learning Robotic Manipulation Skills From Humans</h1>
<blockquote>
<p><strong>标题</strong>：从人类身上理解和学习机器人操作技能<br>
<strong>作者团队</strong>：Stanford University<br>
<strong>期刊会议</strong>：Thesis<br>
<strong>时间</strong>：2022<br>
<strong>代码</strong>：</p>
</blockquote>
<h2 id="5-1-背景和动机">5.1 背景和动机</h2>
<p>制造机器人的性能是通过它们的精度、准确性和速度来衡量的。这导致了刚性和笨重的机器人的设计，这些机器人与人类一起工作是不安全的。他们的控制器在不使用力传感的情况下执行预先编程的轨迹，使其对位置误差高度敏感。通过使用夹具和夹具，例如装配线上的夹具，可以减少环境中的不确定性。</p>
<p>现实世界的环境需要低重量、人类安全、扭矩控制的机器人。<strong>如果机器人要在环境不断变化、感知能力有限的日常环境中真正发挥作用，就必须找到通过预编程轨迹控制机器人的替代方案。一种很有前途的方法是将复杂的任务划分为健壮且可重用的动作或基元。</strong> 在本文中，我们通过使用可推广的顺应原语，为在更高抽象级别上编程机器人奠定了理论和实践基础。</p>
<p><strong>方法的第一步是从人类演示中收集数据。然后，我们将数据分割成在任务期间执行的动作序列——基元。接下来，我们将数据投影到一个低维和物理意义的空间中，使我们能够理解人类的策略。最后，我们将这些行为编码到能够执行任务的机器人控制器上。</strong> 此外，我们的框架利用视觉和触觉反馈，让人类处于故障恢复和持续学习的循环中。</p>
<h2 id="5-2-从人类演示中学习">5.2 从人类演示中学习</h2>
<p><strong>本文的工作属于示范学习LfD的范畴。人类在操作方面非常有能力，因此从人类演示中收集数据使学习机器人新行为的一种流行方式。事实上，我们不仅可以学习单臂行为，还可以学习双臂行为，我们的系统已经证明了这一点，并在其它工作中进行了探索。</strong> 大多数先前的工作侧重于从视觉数据中学习。而我们的工作强调在执行富含接触的任务时里和数据的重要性。</p>
<p>近年来，互动学习是一个不断发展的研究领域，它使人类保持在循环学习的过程中。为了实现类似的工作方式，我们的框架通过使用触觉接口使人类处于循环中，我们系统手机故障恢复数据可以与从故障中学习的工作相结合，易产生更稳健的自主行为。</p>
<h2 id="5-3-机器人基本单元">5.3 机器人基本单元</h2>
<p>在这项研究中，基元是有一个兼容的框架和一组所需的任务参数定义的。顺应性框架是一个原点和空间中的三个方向，我们沿着它们控制运动和顺应性。柔顺框架附着到要操纵的对象上。任务参数包括所需的力、力矩、位置和方向。这种与机器人无关的任务规范提供了一种有物理意义的低维表示。</p>
<p><strong>基元库。生成一个由n个基元组成的库，对基本的操作技能进行编码，通过组合这些基元，可以以一种方式解决新的复杂任务，即所需基元的数量不会随着任务数量的增加而增加。广义上讲，关于运动基元的文献主要解决了三个主要的研究问题，生成运动基元，参数化基元以及将基元组合在一起以成功完成任务。</strong></p>
<p><strong>基元生成。基元的生成可以通过手动编码所需策略或者从数据中自动提取策略来实现。先前的研究已经转向人类寻求灵感，并试图提取策略。</strong></p>
<p><strong>基元参数化。参数化基元处理定义动作的方式。基元通常使用轨迹段进行参数化。用轨迹定义运动基元已经被证明是成功的，但该方法假设环境不确定性较低。</strong></p>
<p>兼容基元。我们使用框架的概念来参数化我们的原始控制器。先前的研究使用了以对象为中心的任务控制器的相同概念。然而，与本文中的工作相反，仅从视觉数据中提取控制器参数，我们认为，在处理复杂任务是，考虑序列数据是有利的。在存在位置不确定性的情况下，顺从性在任务中也起着重要作用，对于接触丰富的任务，比如抓获或本文中研究的任务。例如，基元的概念，其中柔顺基元就是用于实现对小物体的鲁棒抓取。</p>
<p><strong>使用基元进行规划。组合运动基元的概率方法利用了决策过程的固有不确定性，这些方法可以是完全自动化的，也可以是使用混合的方法，将自动决策算法与用户指定的图像相结合。</strong> 最近的其他方法使用语义模型来学习基于是觉得操纵任务计划，或者一些方法使用接触而不是视觉来指导决策过程。</p>
<h2 id="5-4-多层控制体系结构">5.4 多层控制体系结构</h2>
<p>该体系结构由三层感知-动作反馈回路组成。每个层都以不同的抽象级别运行，并以不同的频率运行。</p>
<p>在最底层，完全依赖于控制器，并有助于高速率感官反馈和控制的集成，以实现安全和可预测的机器人运动。该级别向机器人电机发送命令，因此，感知动作回路必须以非常高的频率闭环。下一层向全身控制器提供输入，从而以较慢的速率运行。最后，执行计算成本高昂的感知和规划的最高抽象级别以最低的速率运行。</p>
<p>全身控制级别使用任务优先级。基于优先级的控制使我们在设计原始动作时能够专注于对象及其几何约束。完整的机器人行为可以被视为由具有不同优先级的不同任务组成。例如，高优先级任务可以是避免奇异配置，另一个任务可以处理障碍或摩擦约束。以类似的方式，有一项任务专门负责实现操纵对象之间所需的几何关系。此任务被编码为基元。换句话说，基本动作只涉及对象，因为任务的所有其他方面，包括非几何约束和机器人运动学，都由控制器的其他组件处理。</p>
<p>基于优先级的全身控制使用零空间投影来确保满足所有不同的约束。此外，操作空间公式——使用Jacobian的动态解耦逆来计算递归零空间投影——确保具有不同优先级的任务动态解耦。先前的工作也在操作原语的上下文中使用了这种分层框架。</p>
<h1>6 Scaling Deep Robotic Learning to Broad Real-World Data</h1>
<blockquote>
<p><strong>标题</strong>：将深度机器人学习扩展到广泛的真实世界数据<br>
<strong>作者团队</strong>：Stanford University<br>
<strong>期刊会议</strong>：Thesis<br>
<strong>时间</strong>：2023<br>
<strong>代码</strong>：</p>
</blockquote>
<h2 id="6-1-背景">6.1 背景</h2>
<p>机器人的一个长期梦想是一种通用的家用机器人，它可以被放置在家庭环境中，也许是它以前从未见过的，并执行一系列有用的任务，如煮咖啡、清洁和烹饪。这样一个机器人无疑将在经济上和通过他们的帮助提高人类生活质量方面产生巨大影响。当然，这个梦想仍然是这样，在实现这个目标方面存在着无数的挑战，包括更好的机器人硬件、电池技术和传感。然而，核心挑战之一在于泛化，即机器人在新的物体、环境和任务中取得成功的能力。事实上，人类有这种能力，正是这种能力使我们能够完成像煮一杯咖啡这样的任务，即使在有新厨房和新物体的情况下也是如此。因此，相关的问题仍然存在——我们如何训练我们的机器人，使其能够广泛推广。</p>
<p>解决这个问题的一种方法是利用人类的直觉，以及用于机器人规划和控制的手部设计系统和表示。在这种方式中，人类定义了相关的对象类及其属性和关系（例如，颜色、形状、姿势、上方与下方等），然后可以使用状态估计技术从传感器观测中测量这些量，并且可以使用经典的搜索和运动规划方法来执行任务。然而，至关重要的是，这种方法是基于人类对相关对象、特性的规范，在某些情况下，甚至是每个环境和任务的对象的3D模型，这阻碍了这种方法在存在新对象和环境的情况下容易使用。</p>
<p>有一项工作研究了机器人如何在制造通用机器人时不依赖人类的直觉和手部设计，而是纯粹通过数据和自己的试错来学习行为。具体来说，深度强化学习研究了学习深度神经网络策略的问题，该策略在给定传感器观测的情况下采取行动，从而使策略通过从交互中学习来最大化一些定义的奖励。原则上，这种方法可以让机器人完全靠自己学习技能，而只有少量的成功指标。然而，在实践中，在机器人上运行深度强化学习带来了许多挑战，例如在重置和奖励方面需要人工监督。然而，最关键的是，深度强化学习通常需要在目标环境中进行数百万次在线环境交互才能进行学习，并且一旦完成，所学习的策略只对所训练的环境和数据有效。因此，标准的深度强化学习在消除了人手设计的大部分需求的同时，仍然没有立即让我们更接近能够在新环境和任务中操作的机器人。</p>
<p>退一步看，人们可能会从机器学习的其他领域寻找灵感，特别是过去几年里，自然语言处理和计算机视觉的研究领域取得了巨大进展。主要基于一个简单的配方：：（1）大量、多样化的离线数据集，（2）自监督或廉价监督的训练目标，以及（3）表达性的端到端训练的神经网络模型。这种基础模型的范例特别令人兴奋，因为这些模型表现出了令人印象深刻的泛化——例如，来自ImageNet的视觉模型可以适应癌症检测这样的全新任务，而像BERT这样的预训练语言模型的应用范围从医学编码到视觉问答。事实上，这种概括水平正是我们希望在一个通用机器人中看到的，它可以被放入一个新的环境中，并快速地学会解决新的任务。</p>
<p>所以为什么这个配方还没有在机器人中实现呢？现实世界中的机器人操作带来了许多独特的挑战，这使得直接复制这一配方变得困难——我们既没有足够大和多样化的机器人交互数据集，也不清楚什么类型的学习算法或监督来源可以使我们从这些数据集中大规模学习有用的技能。本文的目标在于解决这些挑战，并在机器人操作的背景下复制大规模数据和学习的配方。具体来说，我的研究集中在回答三个广泛的问题上。首先，我们如何可伸缩地收集在物理世界中交互的机器人的大型和多样化的数据集？其次，我们如何设计能够消耗如此广泛的离线数据的自我监督强化学习算法，这些数据可能来自非专家，缺乏奖励标签，并从中学习达到看不见的目标？第三，我们如何解锁网络上存在的广泛数据来源，如人类视频和自然语言，以便在我们的机器人中进行更有效的学习？</p>
<h1>7 Learning Perceptual Prediction: Learning From Humans and Reasoning About Objects</h1>
<blockquote>
<p><strong>标题</strong>：学习感知预测：向人类学习和对物体的推理<br>
<strong>作者团队</strong>：University of Pennsylvania<br>
<strong>期刊会议</strong>：Thesis<br>
<strong>时间</strong>：2023<br>
<strong>代码</strong>：</p>
</blockquote>
<h2 id="7-1-目标问题-2">7.1 目标问题</h2>
<p>人类在使用各种各样的感知模式进行预测时，主要关注从视觉学习。人类的视觉似乎经过了高度的优化，可以用于预测未来的视觉观测。</p>
<p>研究使用视觉传感器的预测也提供了许多实际优势。首先，高质量的相机很容易获得，并且尺寸、重量和功率要求都很低，这使得它们可以被包括在大多数机器人平台上，由于相机在非机器人应用中的普及，它们已经被商品化了。其次，视觉观察提供了关于环境的丰富信息，包括姿势、纹理和语义，这些信息是其他传感器无法轻易匹配的。获取大量丰富的世界信息对于使代理人能够与世界互动非常重要。</p>
<p>当前学习动作条件视觉预测模型的方法依赖于访问大量的具体数据，这是昂贵且耗时的，从而阻止了基于视觉预测的方法在许多应用中使用。对于机器人来说尤其如此，因为收集大量机器人数据既昂贵又耗时，而且可能不安全。现有工作表明，基于视觉预测的方法随着数据量的增加而扩展良好，因此找到新的数据来源对于使这些模型能够广泛使用至关重要。</p>
<p>在这篇论文中，我提出了三种不同的方法来利用非机器人数据来改进视觉预测和机器人控制。在前两项工作中，我使用人类数据来提高机器人的性能，而在第三项工作中我使用现有的非机器人数据集来实现以对象为中心的预测框架。</p>
<h2 id="7-2-从人类学习">7.2 从人类学习</h2>
<p>大型和多样化的真实世界数据集对于广泛的泛化和高性能至关重要。大型数据集可以通过自动化管道或人类远程操作进行收集。自动化数据收集过程可以收集非常大的数据集，但在到达环境中感兴趣的部分以及需要与环境进行大量交互方面存在问题。习得的探索策略可以提高代理达到有趣配置的能力，但这些方法仍然需要大量的探索。第二种方法是收集人类演示的远程操作轨迹。这种方法允许数据集轻松地达到有趣的和任务相关的配置。然而，它受到了影响，因为它依赖于人类来操作机器人，这需要训练有素的操作员，而且很快变得非常昂贵。通过从人类学习中汲取灵感，可以找到一种避免这两种方法困难的替代方法。</p>
<p>人类不仅有能力从自己与世界的互动中学习技能，也有能力通过观察他人来学习技能。考虑一个婴儿学习使用工具。为了成功地使用一个工具，它需要学习该工具如何与其他对象交互，以及如何移动工具来触发这种交互。这种直观的物理概念可以通过观察成年人如何使用工具来学习。更普遍地说，观察是关于世界以及行动如何导致结果的强大信息来源。然而，在存在身体差异的情况下（例如成人身体和婴儿身体之间），利用观察是具有挑战性的，因为演示者和观察者的行为之间没有直接对应关系。来自神经科学的证据表明，人类可以有效地推断出这种对应关系，并利用它们从观察中学习。</p>
<p>利用对人类的观察提供了大幅增加可用数据的规模和有用性的机会。与自主收集的数据不同，人类数据可以只关注配置空间中有趣的部分，避免危险或无聊的交互。与通过远程操作收集的数据集不同，人类数据集可以具有更大的规模。公开可用的人类数据集，如Ego4D或SomethingSomething，包含数十万个视频和数千小时的镜头，分布在数百个任务和数十个地点。这些数据集与自主收集的数据集的大小相当]，并比通过远程操作收集的数据集中的大小高出一到两个数量级。更重要的是，从人类的无行动观察中学习，开启了从互联网上公开生成的视频中学习的可能性，比如YouTube上的视频，这些视频提供了更多数量级的数据。目前的方法只能将这些数据的有限子集用于特定任务，但我们的工作为更广泛的利用提供了一步。</p>
<p>在这篇论文中，我们考虑了这样一个问题：主体能否学会利用自己的互动和其他主体的被动观察来解决任务？我们在两个环境中探讨了这个问题，第一个是学习动作条件视觉预测模型，第二个是端到端强化学习策略。</p>
<p>在第3章中，我们提出了一种使用人类的无动作数据和主体自己的探索来执行强化学习的方法。我们提出了克服野外人类数据和模拟机器人数据之间的域转换的方法，将动作添加到无动作的人类数据中，以及估计人类数据的奖励的方法。通过利用在现实世界中收集的人类视频，我们能够加快模拟机器人代理的学习速度。</p>
<h2 id="7-3-对象推理">7.3 对象推理</h2>
<p>虽然从人类观察中学习可以获得大量数据，但它并不能回答应该学习什么的问题。我们重点学习端到端模型，这些模型直接从传感器输入映射到预测的未来帧或内部结构很少的期望动作。通过为任务选择正确的归纳偏差集，并利用在非机器人数据上预训练的现有模型，我们应该能够用更少的数据训练我们的模型，并实现更高的性能。正确设计学习问题的结构也可以使模型更容易地用于下游任务。我们关注的是假设世界是由物体组成的简单归纳偏见。</p>
<p>大多数用于操纵的动态交互可以通过将场景分解为对象来建模。虽然有些材料，如液体或颗粒介质，不容易被表示为对象，但大多数操作任务都涉及操作离散对象。分拣箱子、重组房间，甚至喝杯咖啡，这些任务主要由与离散对象的交互控制。</p>
<p>以对象为中心的预测模型在预测和困难任务方面表现出了成功的性能。通过将场景分解为离散对象，这些预测模型可以在更长的时间范围内保持每个对象的内聚性。此外，通过将世界状态内部表示为对象集合，以对象为中心的预测模型可以轻松地与规划者对接，并提供一个非常可解释的界面，有助于调试和验证。</p>
<h1>8 Affordances from Human Videos as a Versatile Representation for Robotics</h1>
<blockquote>
<p><strong>标题</strong>：人类视频作为机器人的通用表示<br>
<strong>作者团队</strong>：CMU, Meta AI<br>
<strong>期刊会议</strong>：CoRL<br>
<strong>时间</strong>：2023<br>
<strong>代码</strong>：<a target="_blank" rel="noopener" href="https://robo-affordances.github.io/">https://robo-affordances.github.io/</a></p>
</blockquote>
<h2 id="8-1-目标问题-2">8.1 目标问题</h2>
<p>从人类视频中学习可操作的动作表示，该模型在未来帧的监督下预测接触点和轨迹路径点。</p>
<p>论文主要关注三个问题：</p>
<ol>
<li>如何表示可操作性？</li>
<li>如何以数据驱动和可扩展的方式学习这种表示？</li>
<li>如何实现跨机器人的视觉启发的方法部署？</li>
</ol>
<p>对应这三个问题，本文提出了以下三种观点：</p>
<ol>
<li>接触点和接触之后的轨迹是比较好的机器人操作的表示方法；</li>
<li>利用了自我中心的数据集，聚焦于所有有人类的帧，来预测接触点和接触之后的轨迹，通过使用现成的工具来估计自我运动、人体姿势和手-物体交互；</li>
<li>实现了一种称为Vision-Robotics Bridge(VRB)的方法，实现这些功能与不同类型机器人的无缝集成。</li>
</ol>
<h2 id="8-2-方法-2">8.2 方法</h2>
<p><strong>（1）可操作性表示</strong></p>
<p>提取人类启示的最自然的方式是观察人们如何与世界互动。常规的思路是从视频中准确模拟人类的运动，但这导致了一个以人为中心的模型，很不容易推广，因为人类的形态和机器人完全不同。</p>
<p>因此本文采用机器人需求驱动的第一性原理，机器人的本体的信息通常是已知的，因此使用运动规划达到3D空间中的点是很容易实现的，关键难点在于与环境的互动位置在哪里，以及接触之后如何移动。</p>
<p>受此启发，采用接触点 $c$ 和接触之后的轨迹 $\tau$ 作为视觉启发的简单操作表示，可以很容易的传递给机器人。其中 $\tau=f(I_t,h_t)$，$I_t$ 是时间步长 $t$ 的图像，$h_t$ 是像素空间中人手位置。</p>
<p><strong>（2）从自我中心的视频中学习操作</strong></p>
<p>接下来的问题是如何处理视觉输入的人体或手，从人类视频中提取接触点 $c$ 和轨迹 $\tau$。</p>
<ol>
<li>从人类视频中提取操作</li>
</ol>
<p>对于给定的视频 $V$，例如人开门，使用现有的手部对象检测模型，对每一帧图像 $I_t$ 生成手的2D边界框和离散接触变量 $o_t$，使用这些信息，我们可以过滤每个图像中 $o_t$ 表示接触的帧，从而找到发生<strong>接触的第一个时间步长</strong> $t_{contanct}$。</p>
<p><strong>手的像素空间位置构成了接触之后的轨迹</strong> $\tau$，为了提取接触点，我们使用手边界框以及颜色分割来找到手与其他物体边界框相交的点，利用高斯混合模型拟合这些接触点。</p>
<p>同时要考虑，一个人打开门的时候，人手不仅会移动，<strong>相机也会移动</strong>，需要补偿相机的运动，使用但应矩阵来解决这一问题，通过匹配连续帧之间的特征来获得单应性矩阵，产生变换后的轨迹。</p>
<p>需要完成<strong>视觉的转移</strong>，即训练视频中包含人手，但机器人任务中的视角不会有，因此考虑将所有的可操作性映射回第一帧，即人类还没有进入场景时。如果人总在视频中，要么将人裁剪出去，要么丢弃。</p>
<ol start="2">
<li>训练模型</li>
</ol>
<p>以输入图像为条件，训练模型预测接触点和接触后的轨迹。然而由于学习的任务是多模态的，比如人从桌子上拿起杯子可能是要喝水或者倒到其他地方，因此考虑建立空间概率分布，预测多个heatmap处理这一问题。</p>
<p>输入图像使用ResNet进行编码，给出潜在空间表示，然后使用卷积层将这个潜在表示投影到K个概率分割中，得到GMM均值的标签的估计。</p>
<p><img src="https://img.mahaofei.com/img/202311011426427.png" alt="image.png"></p>
<p>为了估计接触后的轨迹，本文使用基于Transformer的预测。给定场景中，人类可能与许多对象进行交互，这些对象可能不存在在训练数据中，因此我们通过对接触点周围进行采样来解决，实现更好的泛化。</p>
<p><strong>（3）机器人学习</strong></p>
<p>本文用于引导现有的机器人学习方法，考虑了四种不同的机器人模式。</p>
<p><img src="https://img.mahaofei.com/img/202311011430303.png" alt="image.png"></p>
<ol>
<li>离线数据采集中的模仿学习</li>
</ol>
<p>给定一个图像输入，模型产生接触点和轨迹，我们将这一组数据存储在数据集中。收集到足够的数据后，我们使用模仿学习来控制策略，实现特定的任务。</p>
<ol start="2">
<li>自由奖励的探索</li>
</ol>
<p>目标是发现尽可能多的不同技能，然而现实中从头开始探索效率太低了，因为机器人可能会花费大量时间尝试探索，但仍然无法学习有意义的技能来解决人类想要的任务。</p>
<p>我们考虑先收集数据，然后对所有轨迹进行排序。对于后续的数据采集从高度探索性的轨迹开始进行引导，进一步探索。</p>
<ol start="3">
<li>目标条件的学习</li>
</ol>
<p>利用已知的知识，例如打开的门的图像，监督其进行探索学习。</p>
<ol start="4">
<li>可操作性作为动作空间</li>
</ol>
<p>将机器人在连续空间中的操作，以空间的方式进行参数化，为每个位置分配一个基元。通过学习获得大量预测，利用GMM拟合到这些轨迹点上，获得离散的接触点和轨迹，机器人只需要在这个空间上进行搜索。</p>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>【模仿动作】从人类演示中学习机器人动作规划方法</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.mahaofei.com/post/lfd-motion-papers.html">https://www.mahaofei.com/post/lfd-motion-papers.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>马浩飞</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-09-07</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2023-09-07</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%A8%A1%E4%BB%BF/">模仿</a><a class="post-meta__tags" href="/tags/%E8%A7%86%E8%A7%89/">视觉</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A8%E4%BD%9C/">机器人动作</a></div><div class="post_share"><div class="social-share" data-image="https://img.mahaofei.com/img/202309071622980.png" data-sites="qq,wechat,weibo,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/lfd-grasp-papers.html"><img class="prev-cover" src="https://img.mahaofei.com/img/202308131124867.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【模仿抓取】从人类演示中学习机械臂抓取</div></div></a></div><div class="next-post pull-right"><a href="/post/mediapipe.html"><img class="next-cover" src="https://img.mahaofei.com/img/202312050848155.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Google_Mediapipe关节检测框架</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><a href="https://www.mahaofei.com"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="author-info__name">马浩飞</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-solid fa-graduation-cap"></i><span>学术主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HaofeiMa" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mail@mahaofei.com" target="_blank" title="E-Mail"><i class="fa fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">新增了<a target="_blank" rel="noopener" href="https://academic.mahaofei.com/">学术主页</a>！<br>有任何问题欢迎留言评论或邮件联系。<br>E-mail：<a href="mailto:blog@mahaofei.com" style="text-decoration:underline;">blog@mahaofei.com</a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">1 MimicPlay: Long-Horizon Imitation Learning by Watching Human Play</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%9B%AE%E6%A0%87%E9%97%AE%E9%A2%98-4"><span class="toc-text">1.1 目标问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%96%B9%E6%B3%95-4"><span class="toc-text">1.2 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%80%9D%E8%80%83-4"><span class="toc-text">1.3 思考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">2 One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%9B%AE%E6%A0%87%E9%97%AE%E9%A2%98-2"><span class="toc-text">2.1 目标问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%96%B9%E6%B3%95-2"><span class="toc-text">2.2 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E6%80%9D%E8%80%83"><span class="toc-text">2.3 思考</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">3 Waypoint-Based Imitation Learning for Robotic Manipulation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">4 Building Robot Intelligence by Scaling Human Supervision</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF"><span class="toc-text">4.1 研究背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87"><span class="toc-text">4.2 研究目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E6%94%B6%E9%9B%86%E4%BA%BA%E7%B1%BB%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3 收集人类操作数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E4%BB%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E4%BA%BA%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E5%AD%A6%E4%B9%A0%E6%93%8D%E4%BD%9C"><span class="toc-text">4.4 从大规模人类数据集中学习操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E4%BD%BF%E7%94%A8%E4%BA%BA%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA%E8%83%BD%E5%8A%9B%E6%9B%B4%E5%BC%BA%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA"><span class="toc-text">4.5 使用人类数据集构建能力更强的机器人</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">5 Understanding and Learning Robotic Manipulation Skills From Humans</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E8%83%8C%E6%99%AF%E5%92%8C%E5%8A%A8%E6%9C%BA"><span class="toc-text">5.1 背景和动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E4%BB%8E%E4%BA%BA%E7%B1%BB%E6%BC%94%E7%A4%BA%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-text">5.2 从人类演示中学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%9F%BA%E6%9C%AC%E5%8D%95%E5%85%83"><span class="toc-text">5.3 机器人基本单元</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E5%A4%9A%E5%B1%82%E6%8E%A7%E5%88%B6%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-text">5.4 多层控制体系结构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">6 Scaling Deep Robotic Learning to Broad Real-World Data</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E8%83%8C%E6%99%AF"><span class="toc-text">6.1 背景</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">7 Learning Perceptual Prediction: Learning From Humans and Reasoning About Objects</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E7%9B%AE%E6%A0%87%E9%97%AE%E9%A2%98-2"><span class="toc-text">7.1 目标问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E4%BB%8E%E4%BA%BA%E7%B1%BB%E5%AD%A6%E4%B9%A0"><span class="toc-text">7.2 从人类学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E5%AF%B9%E8%B1%A1%E6%8E%A8%E7%90%86"><span class="toc-text">7.3 对象推理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">8 Affordances from Human Videos as a Versatile Representation for Robotics</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E7%9B%AE%E6%A0%87%E9%97%AE%E9%A2%98-2"><span class="toc-text">8.1 目标问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E6%96%B9%E6%B3%95-2"><span class="toc-text">8.2 方法</span></a></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/mimicplay.html" title="【论文复现】MimicPlay从人类演示中学习机器人技能"><img src="https://img.mahaofei.com/img/202310231710757.png" alt="【论文复现】MimicPlay从人类演示中学习机器人技能"></a><div class="content"><a class="title" href="/post/mimicplay.html" title="【论文复现】MimicPlay从人类演示中学习机器人技能">【论文复现】MimicPlay从人类演示中学习机器人技能</a><time datetime="2023-10-21" title="发表于 2023-10-21">2023-10-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/lfd-grasp-papers.html" title="【模仿抓取】从人类演示中学习机械臂抓取"><img src="https://img.mahaofei.com/img/202308131124867.png" alt="【模仿抓取】从人类演示中学习机械臂抓取"></a><div class="content"><a class="title" href="/post/lfd-grasp-papers.html" title="【模仿抓取】从人类演示中学习机械臂抓取">【模仿抓取】从人类演示中学习机械臂抓取</a><time datetime="2023-08-13" title="发表于 2023-08-13">2023-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/mediapipe.html" title="Google_Mediapipe关节检测框架"><img src="https://img.mahaofei.com/img/202312050848155.png" alt="Google_Mediapipe关节检测框架"></a><div class="content"><a class="title" href="/post/mediapipe.html" title="Google_Mediapipe关节检测框架">Google_Mediapipe关节检测框架</a><time datetime="2023-09-18" title="发表于 2023-09-18">2023-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/lfd-rl-papers.html" title="【论文笔记】基于强化学习的机器人动作模仿"><img src="https://img.mahaofei.com/img/202311091616113.png" alt="【论文笔记】基于强化学习的机器人动作模仿"></a><div class="content"><a class="title" href="/post/lfd-rl-papers.html" title="【论文笔记】基于强化学习的机器人动作模仿">【论文笔记】基于强化学习的机器人动作模仿</a><time datetime="2023-11-09" title="发表于 2023-11-09">2023-11-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/rgb-matters.html" title="【抓取姿态估计算法】RGB Matters论文笔记与复现"><img src="https://img.mahaofei.com/img/202304231549478.png" alt="【抓取姿态估计算法】RGB Matters论文笔记与复现"></a><div class="content"><a class="title" href="/post/rgb-matters.html" title="【抓取姿态估计算法】RGB Matters论文笔记与复现">【抓取姿态估计算法】RGB Matters论文笔记与复现</a><time datetime="2023-05-04" title="发表于 2023-05-04">2023-05-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/contact-graspnet.html" title="【抓取算法】Contact GraspNet"><img src="https://img.mahaofei.com/img/20230404152359.png" alt="【抓取算法】Contact GraspNet"></a><div class="content"><a class="title" href="/post/contact-graspnet.html" title="【抓取算法】Contact GraspNet">【抓取算法】Contact GraspNet</a><time datetime="2023-04-07" title="发表于 2023-04-07">2023-04-07</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img.mahaofei.com/img/GoodnightCopenhagen.png')"><div id="footer-wrap"><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Hosted-Github-brightgreen?style=flat&logo=GitHub"title="本站项目由Gtihub托管"></a><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?logoColor=white&style=flat&logo=buefy"title="主题采用butterfly"></a><a style="margin-inline:5px"target="_blank"href="https://aliyun.com/product/cdn"><img src="https://img.shields.io/badge/DNS-Cloudflare-orange?style=flat&logo=Cloudflare"title="本站使用Cloudflare网络服务"></a><a style="margin-inline:5px"target="_blank"href="https://beian.miit.gov.cn/"><img src="https://img.shields.io/badge/%E6%B4%A5ICP%E5%A4%87-2021000769%E5%8F%B7--3-red?logo=data:image/png;base64,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"title="备案号:津ICP备2021000769号-3"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><i class="fas fa-adjust"><use id="modeicon" xlink:href="#icon-moon"></use></i></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Algolia</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.vemoji)'))
      }
    }, "https://cdn.jsdelivr.net/gh/zhheo/twikoo@dev/dist/twikoo.all.min.js"))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      document.getElementById('twikoo-count').innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.min.js"></script><script src="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/lib/index.js"></script><script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script><script src="/js/custom/categoryBar.js"></script><script src="/js/custom/cardHistory.js"></script><script src="/js/custom/light_dark.js"></script><script src="/js/custom/forbidCopy.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"Jfmc1FSFWs09EC8r",ck:"Jfmc1FSFWs09EC8r"})</script><script src="https://sdk.51.la/perf/js-sdk-perf.min.js" crossorigin="anonymous"></script><script>new LingQue.Monitor().init({id:"JfmcYvlVsVAZlVkS"});</script><script defer src="https://cloud.umami.is/script.js" data-website-id="86b466f8-e8bc-415b-954e-289b3d0110fb"></script><script src="/js/custom/custom.js"></script><script src="/js/custom/nav.js"></script><script id="canvas_nest" defer="defer" color="66,66,66" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>