<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【深度学习笔记01】神经网络与深度学习 | 马浩飞丨博客</title><meta name="keywords" content="深度学习,笔记"><meta name="author" content="马浩飞"><meta name="copyright" content="马浩飞"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="笔者从零开始学习深度学习的笔记，本章介绍深度学习最最底层的数学思路，以及基于回归的简单的深度学习的算法思路和相关代码。">
<meta property="og:type" content="article">
<meta property="og:title" content="【深度学习笔记01】神经网络与深度学习">
<meta property="og:url" content="https://www.mahaofei.com/post/deeplearning-note1.html">
<meta property="og:site_name" content="马浩飞丨博客">
<meta property="og:description" content="笔者从零开始学习深度学习的笔记，本章介绍深度学习最最底层的数学思路，以及基于回归的简单的深度学习的算法思路和相关代码。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.mahaofei.com/img/20220426114017.png">
<meta property="article:published_time" content="2022-05-20T08:33:28.000Z">
<meta property="article:modified_time" content="2022-05-20T08:33:28.000Z">
<meta property="article:author" content="马浩飞">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.mahaofei.com/img/20220426114017.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.mahaofei.com/post/deeplearning-note1"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-bB89NudWgv"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?439a0d0abeb31dd8f338efd8266c999b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ABY1KMOQQM","apiKey":"d3f3a4fbb355106e6bf265cf8da1863b","indexName":"hexo","hits":{"per_page":4},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【深度学习笔记01】神经网络与深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-05-20 16:33:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://unpkg.com/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="/css/custom/MainColor.css"><link rel="stylesheet" href="/css/custom/categoryBar.css"><link rel="stylesheet" href="/css/custom/404.css"><link rel="stylesheet" href="/css/custom/cardHistory.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><link rel="stylesheet" href="/css/custom/custom.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="马浩飞丨博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.mahaofei.com/img/20220426114017.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">马浩飞丨博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【深度学习笔记01】神经网络与深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-05-20T08:33:28.000Z" title="发表于 2022-05-20 16:33:28">2022-05-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-05-20T08:33:28.000Z" title="更新于 2022-05-20 16:33:28">2022-05-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>44分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【深度学习笔记01】神经网络与深度学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/deeplearning-note1.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>一、深度学习概论</h1>
<h2 id="1-1-神经网络">1.1 神经网络</h2>
<p>激活函数ReLU（基本修正单元Rectified linear unit），一开始是零，后面沿直线上升。</p>
<p>一个神经网络中每一个单元都可能是ReLU或者其他非线性单元。</p>
<h2 id="1-2-监督学习">1.2 监督学习</h2>
<p><strong>（1）深度学习的应用领域</strong></p>
<p>在监督学习中，先输入X，然后学习到一个函数，映射输出到Y。</p>
<p>目前深度学习在线上广告投放、图像处理、语音识别、机器翻译、无人驾驶等领域已经发挥了重要的作用。但每一项应用都需要我们合理选择x和y，才能解决特定问题。</p>
<p><strong>（2）一些常用框架</strong></p>
<p>价格预测，<strong>通用标准的神经网络框（Standard NN）</strong><br>
图像处理，<strong>卷积神经网络CNN（Convolutional NN）</strong><br>
序列处理，如音频，<strong>循环神经网络RNN（Recurrent NN）</strong><br>
语言处理，使用更复杂的<strong>RNNs</strong><br>
无人驾驶中使用图像和雷达，会使用更复杂的<strong>混合的神经网络结构</strong></p>
<p><strong>（3）结构化数据与非结构化数据</strong></p>
<p>结构化数据，是数据的数据库，例如价格预测时会有标准的数据库，广告投放时会有用户信息，广告信息等数据库。</p>
<p>非结构化数据指语音、图像、视频、文本等，相比较结构化数据，更难以让计算机理解。</p>
<h2 id="1-3-关于深度学习">1.3 关于深度学习</h2>
<p>深度学习近年强势发展的主要因素。</p>
<p>以前的算法，比如传统机器学习向量机等，随着数据量的增大，算法的表现基本没有提高。早期数据很少，随着数据的增多，算法有了一些提高，但后期人们进入数据化时代，每个人每天都在产生大量数据，数据量越来越大，但以前的算法却并没有得到更好的结果。</p>
<p>这时如果使用数据训练一个小型的神经网络，算法表现会比以前好很多，如果训练一个更复杂的大型神经网络，那么得到的结果会更好。虽然数据量增大会使训练时间增高，但如今大型神经网络已经帮著人类取得了很多很多成果。</p>
<p>另一方面，以前的SVM向量机的训练好坏更却决于认为手动设计，二现在神经网络更加通用。</p>
<p>其次是计算能力，CPU和GPU运算能力的提升也是深度学习发展的基础。</p>
<p>算法方面的创新也极大地促进了深度学习，许多算法的提出都是为了改善神经网络的性能，增加计算速度。</p>
<h1>二、神经网络编程基础</h1>
<h2 id="2-1-符号定义">2.1 符号定义</h2>
<p>对于想要遍历数据集的情况，尽量不要使用for循环。</p>
<p>例如识别一张猫的图片是否是猫。</p>
<p>首先我们获取一张图片，其由3个矩阵（RGB）组成，我们可以将每一个矩阵所有像素值提取出来列成一个特征向量。这样输入的矩阵维度就是3xheightxwidth。</p>
<p>输入就是特征向量，输出就是标签0/1代表有猫还是没猫。</p>
<p>用数字符号表示为如下：</p>
<p>输入与输出：$(x,y)$</p>
<p>数据集：$m:{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$</p>
<p>通常会将输入数据$x^{(n)}$以列向量的形式构建为一个输入矩阵</p>
<p>$X=[x^{(1)},x^{(2)},…,x^{(m)}]$</p>
<p>输入矩阵$X\in{R^{n_x\times{m}}}$，其行列数为$X.shape=(nx,m)$。</p>
<p>同样输出数据$y^{(n)}$也会构建为一个输出矩阵</p>
<p>$Y=[y^{(1)},y^{(2)},…,y^{(m)}]$</p>
<p>输入矩阵$Y\in{R^{1\times{m}}}$，其行列数为$Y.shape=(1,m)$。</p>
<h2 id="2-2-二元分类问题——logistic回归方法">2.2 二元分类问题——logistic回归方法</h2>
<p><strong>（1）二元分类问题</strong></p>
<p>对于一个神经网络的问题，比如图像目标识别，输出的结果$y\in(0,1)$，即图片要么是目标，要么不是。</p>
<p>我们给定了图片输入$x$特征向量，期望得到一个预测值$\hat{y}$，判断图片是不是我们想要的目标，预测的过程相当于求一个概率：</p>
<p>$\hat{y}=P(y=1|x)$</p>
<p><strong>（2）logistic方法</strong></p>
<p>由于二元分类问题，预测结果应为[0,1]，因此线性方式一般不使用，因为线性表达，Y可以无穷大也。常用的方法是在线性的基础上再使用sigmoid函数。</p>
<p>$\textcolor{red}{\hat{y}=\sigma(w^Tx+b)}$</p>
<p>其中$w^T\in{R^{n_x}}$是$x$的系数向量，$b\in{R}$是常数，是一个拦截器，$\sigma(z)=\frac{1}{1+e^{-z}}$是sigmoid函数。利用$\sigma(z)$就可以将直线输出转换为[0,1]输出。</p>
<p><strong>（3）损失函数Loss</strong></p>
<p><strong>损失函数Loss是单个样本的预测值与实际值之差</strong></p>
<p>为了训练$w$和$b$，我们需要定义一个损失函数，用来描述$\hat{y}$与$y$的接近程度。再logistic方法中我们使用下面的函数作为损失函数Loss。</p>
<p>$\textcolor{red}{L({\hat{y},y})=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))}$</p>
<p>对于损失函数，我们希望它越小越好。</p>
<p>当$y=1$时，$L(\hat{y},y)=-\log\hat{y}$，可以看出来$\hat{y}$越大($\hat{y}\rightarrow1$)，Loss越小。</p>
<p>同理$y=0$时，$L(\hat{y},y)=-\log{(1-\hat{y})}$，可以看出来$\hat{y}$越小($\hat{y}\rightarrow0$)，Loss越小。</p>
<p>（再其他方法中可能会用方差作为Loss函数，这里不用是因为方差得到的结果是凹凸的，也就是会有多个局部最优解，不便于后续梯度下降求全局最优解，而上面的Loss函数则解决了这个问题）</p>
<p><strong>（4）成本函数</strong></p>
<p><strong>成本函数是全体样本的预测值与实际值之差</strong></p>
<p>$\textcolor{red}{J(w,b)=\frac{1}{m}\sum^m_{i=1}{L(\hat{y}^{(i)},y^{(i)})}=-\frac{1}{m}(y\log\hat{y}+(1-y)\log(1-\hat{y}))}$</p>
<h2 id="2-3-梯度下降法">2.3 梯度下降法</h2>
<p>成本函数衡量了训练集的预测效果，我们想要的时找到合适的 $w,b$ 使得成本函数 $J(w,b)$ 尽可能小，这就用到了梯度下降法。</p>
<p>由于成本函数 $J(w,b)$ 是凸函数，因此函数形状是如下图这样的，这也是我们为什么定义损失函数那样形式的原因。</p>
<p><strong>我们要做的是初始化一个 $w,b$ 的值，然后让其向梯度下降的方向走，直到找到梯度最低的点。</strong></p>
<p>以 $w$ 为例，我们将重复以下过程：$w=w-\alpha\frac{dJ(w)}{dw}$，来更新 $w$ ，其中 $\alpha$ 是学习率参数，可以看出如果斜率越大，那么每次迭代w变化也越大。</p>
<p>实际中我们会使用下面的方程进行梯度下降求解 $w,b$</p>
<p>$w=w-\alpha\frac{\partial{J(w,b)}}{\partial{w}}$</p>
<p>$b=b-\alpha\frac{\partial{J(w,b)}}{\partial{b}}$</p>
<p><img src="https://img.mahaofei.com/img/20220426114017.png" alt=""></p>
<h2 id="2-4-计算图">2.4 计算图</h2>
<p><strong>计算图</strong>是从左到右的计算，来计算成本函数 $J$。</p>
<p>对于一个流程图，可以很容易的看出 $J$ 的导数。<strong>反向传播</strong>，就是从最终输出反推 $J$ 对各个中间变量以及输入的导数。</p>
<p>在程序里，通常约定编程时使用<code>dvar</code>代表最终输出变量对于变量<code>var</code>的导数</p>
<p>代码大致过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">J = <span class="number">0</span></span><br><span class="line">dw1 = <span class="number">0</span></span><br><span class="line">dw2 = <span class="number">0</span></span><br><span class="line">db = <span class="number">0</span></span><br><span class="line"><span class="comment"># 遍历数据集</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">	<span class="comment"># 计算损失函数与成本函数</span></span><br><span class="line">	zi = w1 * x1 + w2 * x2 +b</span><br><span class="line">	ai = <span class="number">1</span>/(<span class="number">1</span>+exp(-zi))</span><br><span class="line">	J += -[yi * log(ai) + (<span class="number">1</span> - yi) * log(<span class="number">1</span>-ai)]</span><br><span class="line">	<span class="comment"># 求导</span></span><br><span class="line">	dzi = ai -yi</span><br><span class="line">	dw1 += x1 * dzi</span><br><span class="line">	dw2 += x2 * dzi</span><br><span class="line">	db += dzi</span><br><span class="line"><span class="comment"># 解出各参数优化后的值</span></span><br><span class="line">J/=m</span><br><span class="line">dw1/=m</span><br><span class="line">dw2/=m</span><br><span class="line">db/=m</span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line">w1=w1-a*dw1</span><br><span class="line">w2=w2-a*dw2</span><br><span class="line">b=b-a*db</span><br></pre></td></tr></table></figure>
<h2 id="2-5-向量化">2.5 向量化</h2>
<p><strong>（1）什么是向量化</strong></p>
<p>由于输入数据$x$和系数$w$都是列向量，对于这两个向量相乘，如果使用非向量化的方法即for循环实现，运算速度非常慢，而如果使用向量化的方法，例如在numpy中使用<code>z=np.dot(w,x)</code>实现两个向量的点乘，运算速度会快的多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量化</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vectorized:&quot;</span> + <span class="built_in">str</span>(<span class="number">1000</span>*(toc-tic))+<span class="string">&quot;ms&quot;</span>)</span><br><span class="line"><span class="comment"># 非向量化</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">	c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;For Loop:&quot;</span> + <span class="built_in">str</span>(<span class="number">1000</span>*(toc-tic))+<span class="string">&quot;ms&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果向量化的方法用时1.5ms，for循环方法500ms</span></span><br></pre></td></tr></table></figure>
<p>对于这个小算法区分还不明显，但如果你训练一个神经网络使用非向量化方法需要300小时，向量化方法只需要1个小时，那样差距就非常明显了。</p>
<p><strong>（2）前向传播的向量化</strong></p>
<p>前向传播的目的是计算$z^{(i)}$和$a^{(i)}$</p>
<p>$z^{(i)}=w^Tx^{(i)}+b$<br>
$a^{(i)}=\sigma(z^{(i)})$</p>
<p>可以通过以下方式实现向量化：<br>
$Z=[z^{(1)}\ z^{(2)}\ \cdots\ z^{(m)}]=w^T\cdot{X}+[b\ b\ \cdots\ b]=[w^Tx^{(1)}+b\ w^Tx^{(2)}+b\ \cdots\ w^Tx^{(m)}+b]$<br>
其中$X$是输入矩阵，每列是每一个样本的所有特征，共有m列即共m个样本；$w^T$是参数$w^{(i)}\rightarrow w^{(nx)}$组成的行向量。</p>
<p>以<code>python</code>程序表示上述过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z=np.dot(w.T, X) + b</span><br></pre></td></tr></table></figure>
<p><strong>（3）梯度计算的向量化</strong></p>
<p>梯度计算就是计算$dz^{(i)}$、$dw^{(i)}$和$db^{(i)}$</p>
<p>$dz^{(i)}=a^{(i)}-y^{(i)}$<br>
$dw^{(i)}=x^{(i)}dz^{(i)}$<br>
$db^{(i)}=dz^{(i)}$</p>
<p>可以通过以下方式实现向量化：</p>
<p>将$dz^{(i)}$, $dw^{(i)}$和$db^{(i)}$向量化：</p>
<p>$dw=[dw^{(1)}\ dw^{(2)}\ \cdots\ dw^{(m)}]$<br>
$db=[db^{(1)}\ db^{(2)}\ \cdots\ db^{(m)}]$<br>
$dz=[dz^{(1)}\ dz^{(2)}\ \cdots\ dz^{(m)}]$</p>
<p>则$db=\frac{1}{m}\sum^{m}_{i=1}dz^{(i)}$, $dw=\frac{1}{m}Xdz^T$</p>
<p>以<code>python</code>表达</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z=np.dot(w.T,x)+b</span><br><span class="line">A=sigmod(Z)</span><br><span class="line">dZ=A-Y</span><br><span class="line">db=np.<span class="built_in">sum</span>(dZ)/m</span><br><span class="line">dw=np.dot(X,dZ^T)/m</span><br><span class="line">w=w-a1*dw</span><br><span class="line">b=b-a2*db</span><br></pre></td></tr></table></figure>
<p><strong>（4）总结</strong><br>
为了加快代码计算速度，我们使用了向量化的手段，即尽量使用numpy计算数据集而不是用for循环。</p>
<p>输入矩阵 $X.shape = (nx,m)$，$m$个样本，每个样本$nx$个特征</p>
<p>$X=[x^{(1)}\ x^{(2)}\ \cdots\ x^{(m)}]$，其中 $x^{(i)}=[x^{(i)}_1\ x^{(i)}<em>2\ \cdots\ x^{(i)}</em>{nx}]^T$</p>
<p>参数矩阵 $W.shape=(nx,1)$，每个特征对应一个参数$w$</p>
<p>$W=[w_1\ w_2\ \cdots\ w_{nx}]^T$</p>
<p>参数 $B.shape=(1,m)$ ，每个特征对应一个参数$b$</p>
<p>$B=[b\ b\ \cdots\ b]$</p>
<p>线性值 $Z.shape=(1,m)$，相当于简化版 $\hat y$ ，每个样本都会计算得到一个计算值</p>
<p>$Z=W^TX+b=[W^Tx^{(1)}+b\ \ \ \ W^Tx^{(2)}+b\ \ \cdots\ \ W^Tx^{(m)}+b]$</p>
<p>预测值 $A.shape=(1,m)$，每个样本一个预测值</p>
<p>$A=\hat Y=\sigma{(Z)}$</p>
<p>损失函数 $L.shape=(1,m)$，每个样本的预测值与标定值距离</p>
<p>$L=-Y\log{A}-(1-Y)\log{(1-A)}$</p>
<p>成本函数 $J.spape=(1,1)$，所有损失函数平均值</p>
<p>$J=\frac{1}{m}L.sum()$</p>
<p>导数 $dZ.shape=(1,m)$</p>
<p>$dZ=\frac{dJ}{dZ}=A-Y$</p>
<p>导数 $dW.shape=(nx,1)$</p>
<p>$dW=\frac{1}{m}X\cdot dZ=[\frac{1}{m}dw^{(1)}\ \frac{1}{m}dw^{(2)}\ \cdots\ \frac{1}{m}dw^{(nx)}]^T$</p>
<p>导数 $dB.shape=(1,1)$</p>
<p>$db=\frac{1}{m}dZ.sum()$</p>
<h2 id="2-6-关于numpy与常用函数">2.6 关于numpy与常用函数</h2>
<p>最好不要使用秩为1的数组<code>a=np.random.randn(5)</code>，而要使用$(m,1)$或$(1,n)$的矩阵。也就是直接定义成行列向量<code>a=np.random.randn(5,1)</code>，或者使用<code>reshape</code>改变形状。</p>
<p>如果不确定某个向量的维度，可以使用assert声明来判断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape==(<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>（1）sigmod函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    x -- 任意形状的numpy数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    s=<span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p><strong>（2）梯度计算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    x -- numpy数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    ds -- 计算的梯度值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    ds = s*(<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>
<p>**（3）输入图片变形为[width x height x 3, 1]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">image2vector</span>(<span class="params">image</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    image -- 特定形状的numpy数组 shape(length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    v -- 向量形式的数组 shape(length*height*depth, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<p><strong>（4）归一化处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normalizeRows</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    x -- 一个二维numpy数组 shape(n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    x -- 按行归一化后的numpy矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算x数组的模</span></span><br><span class="line">    x_norm = np.linalg.norm(x,<span class="built_in">ord</span>=<span class="number">2</span>,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 用x除以它的模</span></span><br><span class="line">    x=x/x_norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>计算方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>ord=默认</td>
<td>二范数：$l_2$</td>
<td>$\sqrt{x_1^2+x_2^2+\dots+x_n^2}$</td>
</tr>
<tr>
<td>ord=2</td>
<td>二范数：$l_2$</td>
<td>同上</td>
</tr>
<tr>
<td>ord=1</td>
<td>一范数：$l_1$</td>
<td>$\lvert x_1\rvert+\lvert x_2\rvert+\dots+\lvert x_n\rvert$</td>
</tr>
<tr>
<td>ord=np.inf</td>
<td>无穷范数：$l_{\infty}$</td>
<td>$MAX\lvert x_i\rvert$</td>
</tr>
<tr>
<td>axis=1</td>
<td>按行向量处理</td>
<td></td>
</tr>
<tr>
<td>axis=0</td>
<td>按列向量处理</td>
<td></td>
</tr>
<tr>
<td>axis=None</td>
<td>矩阵范数</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>（5）softmax函数</strong></p>
<p>softmax函数是一个规范化函数，用于算法需要分类两个或多个类的情况。</p>
<p>$$softmax(x) = softmax\begin{bmatrix}<br>
x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \<br>
x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>
x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}<br>
\end{bmatrix} = \begin{bmatrix}<br>
\frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \<br>
\frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>
\frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}<br>
\end{bmatrix} = \begin{pmatrix}<br>
softmax\text{(first row of x)}  \<br>
softmax\text{(second row of x)} \<br>
…  \<br>
softmax\text{(last row of x)} \<br>
\end{pmatrix} $$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    x -- 一个二维numpy数组 shape(n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    x -- softmax后的numpy矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算exp(x)</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line">    <span class="comment"># 创建向量x_sum对x_exp的每一行求和</span></span><br><span class="line">    x_sum = np.<span class="built_in">sum</span>(x_exp, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 两者相除计算softmax</span></span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<p><strong>（6）Loss函数</strong></p>
<p>L1 Loss函数，用于评价模型的表现，loss越大，说明预测值和真实值的偏差预约。</p>
<p>$$\begin{align*} &amp; L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}\tag{6}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L1</span>(<span class="params">yhat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    yhat -- 长度m的向量（预测值）</span></span><br><span class="line"><span class="string">    y -- 长度m的向量（真实值）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    loss -- 上面定义的L1 Loss值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    loss = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y-yhat))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>L2 Loss函数。</p>
<p>$$\begin{align*} &amp; L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}\tag{7}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L2</span>(<span class="params">yhat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    yhat -- 长度m的向量（预测值）</span></span><br><span class="line"><span class="string">    y -- 长度m的向量（真实值）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    loss -- 上面定义的L2 Loss值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    loss = np.<span class="built_in">sum</span>(np.dot(y-yhat,y-yhat))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="2-7-总结">2.7 总结</h2>
<p><strong>（1）数学过程</strong></p>
<p>二分神经网络目的是给定一个输入 $x$（可以是图片等），输出预测值 $\hat y$，并尽可能让预测值 $\hat y$ 接近实际值 $y$，即：</p>
<p>$\hat{y}=P(y=1|x)$</p>
<p>而输出预测值 $\hat y$ 与输入 $x$ 之间的函数关系可以表示为（在线性表达的基础上加上sigmod函数，保证输出位于[0,1]区间）</p>
<p>$$\hat y=\sigma{(wx+b)}$$</p>
<p>其中定义 $z=wx+b$，$\sigma(z)=\frac{1}{1+e^{-z}}$</p>
<p>这其中我们需要做的就是找到最合适的 $w$ 和 $b$ ，使输出 $\hat y$ 尽可能接近 $y$。</p>
<p>因此为了衡量 $\hat y$ 与 $y$ 的接近程度，我们定义了一个损失函数Loss，只要让损失函数足够小，就能保证 $\hat y$ 与 $y$ 足够接近。</p>
<p>$$L(\hat y,y)=-y\log{\hat y}-(1-y)\log{(1-\hat y)}$$</p>
<p>当 $y=1$ 时， $L(\hat y,y)=-\log{\hat y}$，我们想要 $L(\hat y,y)$ 足够小，就要让 $\hat y$ 足够大即 $\hat y\rightarrow y=1$（由于时二分问题，$y\in [0,1]$）<br>
当 $y=0$ 时， $L(\hat y,y)=-\log{(1-\hat y)}$，我们想要 $L(\hat y,y)$ 足够小，就要让 $\hat y$ 足够小即 $\hat y\rightarrow y=0$。</p>
<p>以上就说明了为什么这个损失函数能够描述 $\hat y$ 与 $y$ 的接近程度。</p>
<p>而上面只是一个样本 $x$ 的损失函数，而对于一个问题一般会输入大量的样本，假设有 $m$ 个，对于每一个样本都需要按照上述方法计算一个损失函数，而所有损失函数的平均值就是我们的输入的整体成本函数。</p>
<p>$$J(w,b)=\frac{1}{m}\sum^{n_x}_{i=1}L(\hat y^{(i)},y^{(i)})$$</p>
<p>因此训练的目的就变成了找到让 $J(w,b)$ 的极小值时的 $w$ 和 $b$。</p>
<p>因此每次训练迭代，都需要对成本函数求导 $\frac{\partial J}{\partial w}$ 和 $\frac{\partial J}{\partial b}$，然后根据求导结果更新 $w=w-\alpha \frac{\partial J}{\partial w}$ 和 $b=b-\frac{\partial J}{\partial w}$，直到找到 $J_{min}$ 对应的 $w$ 和 $b$，此时训练就完成了。</p>
<p><strong>（2）代码过程</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> h5py                     <span class="comment"># 常用数据集交互工具，数据集被保存为H5文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy                    <span class="comment"># 用于图片测试</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image          </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集加载函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>():</span><br><span class="line"></span><br><span class="line">    train_dataset = h5py.File(<span class="string">&#x27;datasets/train_catvnoncat.h5&#x27;</span>, <span class="string">&quot;r&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_set_x_orig = np.array(train_dataset[<span class="string">&quot;train_set_x&quot;</span>][:]) <span class="comment"># your train set features</span></span><br><span class="line"></span><br><span class="line">    train_set_y_orig = np.array(train_dataset[<span class="string">&quot;train_set_y&quot;</span>][:]) <span class="comment"># your train set labels</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    test_dataset = h5py.File(<span class="string">&#x27;datasets/test_catvnoncat.h5&#x27;</span>, <span class="string">&quot;r&quot;</span>)</span><br><span class="line"></span><br><span class="line">    test_set_x_orig = np.array(test_dataset[<span class="string">&quot;test_set_x&quot;</span>][:]) <span class="comment"># your test set features</span></span><br><span class="line"></span><br><span class="line">    test_set_y_orig = np.array(test_dataset[<span class="string">&quot;test_set_y&quot;</span>][:]) <span class="comment"># your test set labels</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    classes = np.array(test_dataset[<span class="string">&quot;list_classes&quot;</span>][:]) <span class="comment"># the list of classes</span></span><br><span class="line"></span><br><span class="line">    train_set_y_orig = train_set_y_orig.reshape((<span class="number">1</span>, train_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    test_set_y_orig = test_set_y_orig.reshape((<span class="number">1</span>, test_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    x -- numpy数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    s -- 计算的sigmoid值,sigmoid(z)=1/(1+e^(-z))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数，为w, b创建0向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_with_zeros</span>(<span class="params">dim</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dim -- w向量的长度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    w -- 初始化的向量, w.shape() -&gt; (dim, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    b -- 初始化的标量, 偏置值b</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    w = np.zeros([dim,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(b, <span class="built_in">float</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(b, <span class="built_in">int</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向和后向传播函数，计算成本函数和梯度值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">propagate</span>(<span class="params">w, b, X, Y</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    w -- weights权重, numpy数组 w.shape -&gt; (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    b -- bias偏置, 标量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X -- 数据 X.shape -&gt; (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y -- 标签向量 (0非猫; 1猫) Y.shape -&gt; (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    cost -- 成本函数(逻辑回归的负对数)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dw -- 损失函数对w的梯度,与w维度相同</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    db -- 损失函数对b的梯度,与b维度相同</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播(从数据X获得成本函数cost)</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line"></span><br><span class="line">    cost = - <span class="number">1</span>/m * np.<span class="built_in">sum</span>(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 后向传播(计算梯度)</span></span><br><span class="line"></span><br><span class="line">    dw = <span class="number">1</span>/m * np.dot(X, (A-Y).T)</span><br><span class="line"></span><br><span class="line">    db = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(A-Y)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == <span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">&quot;dw&quot;</span>: dw,</span><br><span class="line"></span><br><span class="line">             <span class="string">&quot;db&quot;</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化函数，通过梯度下降算法优化w和b</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">optimize</span>(<span class="params">w, b, X, Y, num_iterations, learning_rate, print_cost = <span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    w -- weights权重, numpy数组 w.shape -&gt; (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    b -- bias偏置, 标量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X -- 数据 X.shape -&gt; (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y -- 标签向量 (0非猫; 1猫) Y.shape -&gt; (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_iterations -- 优化循环的迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print_cost -- True则每100次打印1次成本函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    params -- 包括 权重w和偏置b 的字典</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    grads -- 包括 成本函数对w和b梯度 的字典</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    costs -- 优化过程中所有的成本函数列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 成本函数和梯度计算</span></span><br><span class="line"></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从grads字典中获取梯度dw和db</span></span><br><span class="line"></span><br><span class="line">        dw = grads[<span class="string">&quot;dw&quot;</span>]</span><br><span class="line"></span><br><span class="line">        db = grads[<span class="string">&quot;db&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新w和b</span></span><br><span class="line"></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line"></span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录成本函数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每100个训练样本打印1次成本函数</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">&quot;w&quot;</span>: w,</span><br><span class="line"></span><br><span class="line">              <span class="string">&quot;b&quot;</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">&quot;dw&quot;</span>: dw,</span><br><span class="line"></span><br><span class="line">             <span class="string">&quot;db&quot;</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测函数，利用学习到的逻辑回归w和b预测标签</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">w, b, X</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    w -- weights权重, numpy数组 w.shape -&gt; (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    b -- bias偏置, 标量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X -- 数据 X.shape -&gt; (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y_prediction -- numpy向量包含对所有X样本的预测值 (0/1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算预测值A，预测图片中的是否是猫</span></span><br><span class="line"></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 转换概率 A[0,i] 到真实的概率 p[0,i]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] &lt; <span class="number">0.5</span>:</span><br><span class="line"></span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 综合以上函数，构建逻辑回归模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = <span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;    </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X_train -- 训练数据 (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y_train -- 训练数据标签 (1, m_train)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X_test -- 测试数据 (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Y_test -- 测试数据标签 (1, m_test)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    num_iterations -- 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    print_cost -- 是否打印成本函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    d -- 包含模型信息的字典</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line"></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line"></span><br><span class="line">    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从params中获得w和b</span></span><br><span class="line"></span><br><span class="line">    w, b = params[<span class="string">&quot;w&quot;</span>], params[<span class="string">&quot;b&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测训练和测试样本</span></span><br><span class="line"></span><br><span class="line">    Y_prediction_train = predict(w, b, X_train);</span><br><span class="line"></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印训练和测试的准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train accuracy: &#123;&#125; %&quot;</span>.<span class="built_in">format</span>(<span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;test accuracy: &#123;&#125; %&quot;</span>.<span class="built_in">format</span>(<span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">&quot;costs&quot;</span>: costs,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;Y_prediction_test&quot;</span>: Y_prediction_test,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;Y_prediction_train&quot;</span> : Y_prediction_train,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;w&quot;</span> : w,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;b&quot;</span> : b,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;learning_rate&quot;</span> : learning_rate,</span><br><span class="line"></span><br><span class="line">         <span class="string">&quot;num_iterations&quot;</span>: num_iterations&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_set_x_orig是训练图片数据，train_set_y是训练标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个图像都是正方形(num_px, num_px, 3)</span></span><br><span class="line"></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集基本参数</span></span><br><span class="line"></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>] <span class="comment"># 训练数据数量</span></span><br><span class="line"></span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]   <span class="comment"># 测试数据数量</span></span><br><span class="line"></span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]  <span class="comment"># 图片长宽</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 展开训练数据集为一维</span></span><br><span class="line"></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T</span><br><span class="line"></span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集标准化处理（将所有像素值[0, 255]映射到[0, 1]）</span></span><br><span class="line"></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行model进行训练</span></span><br><span class="line"></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">20000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某张图片的预测结果</span></span><br><span class="line"></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;y = &quot;</span> + <span class="built_in">str</span>(test_set_y[<span class="number">0</span>,index]) + <span class="string">&quot;, you predicted that it is a \&quot;&quot;</span> + classes[<span class="built_in">int</span>(d[<span class="string">&quot;Y_prediction_test&quot;</span>][<span class="number">0</span>,index])].decode(<span class="string">&quot;utf-8&quot;</span>) +  <span class="string">&quot;\&quot; picture.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出学习曲线</span></span><br><span class="line"></span><br><span class="line">costs = np.squeeze(d[<span class="string">&#x27;costs&#x27;</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(costs)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;iterations (per hundreds)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Learning rate =&quot;</span> + <span class="built_in">str</span>(d[<span class="string">&quot;learning_rate&quot;</span>]))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1>三、神经网络</h1>
<h2 id="3-1-神经网络的表示">3.1 神经网络的表示</h2>
<p>一般的神经网络包括：输入层、隐藏层、输出层、输出值。</p>
<blockquote>
<p>隐藏层：在训练集中，这些节点的数值我们不知道，我们能看到输入值、输出值，但是中间值我们在训练是看不到的，所以叫隐藏层。</p>
</blockquote>
<p><img src="https://img.mahaofei.com/img/20220705151527.png" alt=""></p>
<p>上图就是标准的二层神经网络（输入层不算是标准的层），或者称为单隐层神经网络。</p>
<p>上面的网络中，输入层也可以用$a^{[0]}$表示，$a^{[0]}=X$，中间层可以用$a^{[1]}$表示，$a^{[1]}=[a^{[1]}_1 a^{[1]}_2 \cdots a^{[1]}_n]$，输出层可以用$a^{[2]}$表示。我们用方括号表示神经网络的层数。其中隐藏层和输出层有着各自的参数，分别记作$w^{[1]}, b^{[1]}$和$w{[2]}, b{[2]}$。</p>
<h2 id="3-2-神经网络的计算">3.2 神经网络的计算</h2>
<p>神经网络的计算过程，就是上一节中的回归计算的多次叠加。下图为上一节所说的逻辑回归的流程。</p>
<p><img src="https://img.mahaofei.com/img/20220705153535.png" alt=""></p>
<p>对应于二层神经网络中是如下图的部分（隐藏层与输入层之间），每一个隐层单元的计算过程都是同样的。</p>
<p><img src="https://img.mahaofei.com/img/20220705153640.png" alt=""></p>
<p>输出层与隐藏层之间的计算，也和隐藏层与输入层类似。只不过将隐藏层计算的$a^{[1]}$看作输入来进行回归运算。</p>
<p>用数学公式表示二层神经网络的计算过程：</p>
<p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$<br>
$$a^{[1]}=\sigma (z^{[1]})$$<br>
$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$<br>
$$a^{[2]}=\sigma (z^{[2]})$$</p>
<h2 id="3-3-激活函数">3.3 激活函数</h2>
<p><strong>（1）$\sigma$函数</strong></p>
<p>前面我们提到激活函数，都是使用的$\sigma$函数[0, 1]，但实际中几乎不使用该函数。</p>
<p><img src="https://img-blog.csdnimg.cn/20200306204205328.png" alt=""></p>
<p><strong>（2）tanh函数</strong></p>
<p>更一般的情况下，我们会使用其他非线性函数例如$tanh = \frac{e^z-e^{-z}}{e^z+e^{-z}}$函数[-1, 1]。事实证明，对于隐藏层的激活函数，如果使用tanh效果几乎总比$\sigma$好。对于输出层，由于$\hat y \in [0, 1]$，因此使用$\sigma$函数效果更好。</p>
<p><img src="https://pic4.zhimg.com/v2-9c23cf5defb43ca3a45639a04885d603_b.jpg" alt=""></p>
<p><strong>（3）ReLU函数</strong></p>
<p>但是无论是$\sigma$还是tanh，我们都可以看出在z非常大或非常小时，激活函数的斜率都很小，这会严重影响梯度下降找到最优解的速度。因此人们提出了线性修正单元ReLU。</p>
<p><img src="https://img-blog.csdnimg.cn/20200306215243190.png" alt=""></p>
<p>还有一种带泄露的线性修正单元 Leaky ReLU，在z&lt;0时a不为0。</p>
<p><img src="https://img-blog.csdnimg.cn/20210511133100300.png" alt=""></p>
<p><strong>（4）为什么需要非线性激活函数</strong></p>
<p>如果不使用机器学习，那么之前所说的二层神经网络的前向计算过程：</p>
<p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$<br>
$$a^{[1]}=\sigma (z^{[1]})$$<br>
$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$<br>
$$a^{[2]}=\sigma (z^{[2]})$$<br>
令$a^{[1]}=z^{[1]}$，那么计算过程就可以写成下面的形式：</p>
<p>$$z^{[2]}=W^{[2]}z^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=W’x+b’$$</p>
<p>化简出来$z^{[2]}$与x之间可以线性表示，那么也就是说无论网络有多少层，都可以将输出和输入用一个线性函数表示，那么神经网络的作用就没法体现了。因此激活函数是十分必要的。</p>
<p>从另一个角度来说，神经网络模拟的就是神经元的信号传递过程，ReLU等激活函数，模拟的就是神经元的放电阈值，当电刺激达到一定阈值才会向下一神经元发送信号，如果激活函数达到阈值才会输出，否则就是0，也是为了去掉某些不重要的样本特征，防止出现过拟合等现象。</p>
<p><strong>（5）激活函数的导数</strong></p>
<p>如果是sigmod函数，即$g(z)=\frac{1}{1+e^{-z}}$，则容易计算得到$\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}] (1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))$。</p>
<ul>
<li>z-&gt;+∞时，g(z)-&gt;1，g’(z)-&gt;0；</li>
<li>z-&gt;-∞时，g(z)-&gt;0，g’(z)-&gt;0</li>
<li>z=0是，g(z)=0.5，g’(z)=0.25</li>
</ul>
<p>如果是tanh函数，即$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$，则容易计算得到$\frac{d}{dz}g(z)=1-(tanh(z))^2$。</p>
<ul>
<li>z-&gt;+∞时，g(z)-&gt;1，g’(z)-&gt;0；</li>
<li>z-&gt;-∞时，g(z)-&gt;-1，g’(z)-&gt;0</li>
<li>z=0是，g(z)=0，g’(z)=1</li>
</ul>
<p>如果是ReLU函数，即$g(z)=max(0,z)$，则容易计算得到<br>
$g’(z)=0, if z&lt;0$<br>
$g’(z)=1, if z&gt;=0$<br>
（其中z=0时可以在程序中设置导数为1，避免不可导问题）</p>
<h2 id="3-4-神经网络的梯度下降">3.4 神经网络的梯度下降</h2>
<p>对于一个神经网络，我们有以下参数：$w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}$</p>
<p>我们计算的损失函数：</p>
<p>$J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]})=\frac{1}{m} \sum^n_{i=1}l(\hat y, y)$</p>
<p>梯度下降过程重复以下步骤：</p>
<ol>
<li>计算预测值$\hat y$</li>
<li>计算导数：$d(w^{[1]})=\frac{\partial J}{\partial w^{[1]}}$，$d(b^{[1]})=\frac{\partial J}{\partial b^{[1]}}$</li>
<li>更新参数：$w^{[1]}=w^{[1]}-\alpha \frac{\partial J}{\partial w^{[1]}}$，$b^{[1]}=b^{[1]}-\alpha \frac{\partial J}{\partial b^{[1]}}$</li>
</ol>
<p>向量化之后反向传播的计算方法如下：</p>
<p>$$dZ^{[2]}=A^{[2]}-Y$$<br>
$$dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}$$<br>
$$db^{[2]}=dZ^{[2]}$$<br>
$$dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})$$<br>
$$dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T$$<br>
$$db^{[1]}=dZ^{[1]}$$</p>
<h2 id="3-5-参数初始化">3.5 参数初始化</h2>
<p>对于神经网络的各层权重参数，我们都需要进行初始化，但是如果只是初始化为0，那么神经网络将完全无效，因此我们需要进行随机初始化。</p>
<p>可以令<br>
W[1]=np.random.randn((2,2))*0.01<br>
b[1]=np.zero((2,1))<br>
W[1]=np.random.randn((1,2))*0.01<br>
b[1]=0</p>
<p>乘0.01是因为我们比较喜欢让权重尽可能小，这样z也将会比较小，更容易落在激活函数斜率大的区域，让神经网络回归的更快。</p>
<h2 id="3-6-代码实现">3.6 代码实现</h2>
<p><strong>（1）安装包</strong></p>
<p>在实现神经网络过程中需要导入一些必要的包</p>
<ul>
<li>numpy：基础的科学计算包</li>
<li>sklearn：提供了数据挖掘和数据分析的简单有效的工具</li>
<li>matplotlib：画图工具</li>
</ul>
<p><strong>（2）神经网络</strong></p>
<p>建立神经网络的方法</p>
<ol>
<li>
<p>定义神经网络结构（输入单元数，隐藏单元数等）</p>
</li>
<li>
<p>初始化模型的参数</p>
</li>
<li>
<p>循环：</p>
<ul>
<li>实现前向传播</li>
<li>计算损失函数</li>
<li>后向传播获得梯度</li>
<li>梯度下降更新参数</li>
</ul>
</li>
<li>
<p>定义神经网络结构</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_sizes</span>(<span class="params">X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 输入数据集，shape(input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 标签，shape(output size, number of examples)</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    n_x -- 输入层的大小</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层的大小</span></span><br><span class="line"><span class="string">    n_y -- 输出层的大小</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]</span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>初始化模型参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x, n_h, n_y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    n_x -- 输入层的大小</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层的大小</span></span><br><span class="line"><span class="string">    n_y -- 输出层的大小</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    params -- python字典包含以下参数:</span></span><br><span class="line"><span class="string">		W1 -- 权重矩阵，shape (n_h, n_x)</span></span><br><span class="line"><span class="string">		b1 -- 偏置向量，shape (n_h, 1)</span></span><br><span class="line"><span class="string">		W2 -- 权重矩阵，shape (n_y, n_h)</span></span><br><span class="line"><span class="string">		b2 -- 偏置向量，shape (n_y, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">2</span>) </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>前向传播计算</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_propagation</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 输入数据，size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典 (上面初始化函数的输出)</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    A2 -- 第二次激活后的输出</span></span><br><span class="line"><span class="string">    cache -- 包含 &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot;, &quot;A2&quot; 的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字典 &quot;parameters&quot; 中获取各参数</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播计算A2</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = np.tanh(Z2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    cache = &#123;<span class="string">&quot;Z1&quot;</span>: Z1,</span><br><span class="line">             <span class="string">&quot;A1&quot;</span>: A1,</span><br><span class="line">             <span class="string">&quot;Z2&quot;</span>: Z2,</span><br><span class="line">             <span class="string">&quot;A2&quot;</span>: A2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>计算损失函数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">A2, Y, parameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    A2 -- 第二次激活函数的输出，shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真值向量，shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    cost -- 损失函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># 样本的数量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失函数</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply((<span class="number">1</span> - Y), np.log(<span class="number">1</span> - A2))</span><br><span class="line">    cost = - np.<span class="built_in">sum</span>(logprobs) / m </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># 保证损失函数的维度是我们想要的，例如把[[6]]变成6</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(<span class="built_in">isinstance</span>(cost, <span class="built_in">float</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>反向传播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_propagation</span>(<span class="params">parameters, cache, X, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典</span></span><br><span class="line"><span class="string">    cache -- 包含 &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot;, &quot;A2&quot; 的字典</span></span><br><span class="line"><span class="string">    X -- 输入数据，shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真值向量，shape (1, number of examples)</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数的梯度值的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字典 &quot;parameters&quot; 中获取 W1, W2</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字典 &quot;cache&quot; 中获取 A1, A2</span></span><br><span class="line">    A1 = cache[<span class="string">&quot;A1&quot;</span>]</span><br><span class="line">    A2 = cache[<span class="string">&quot;A2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播: 计算 dW1, db1, dW2, db2.</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A2.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), (<span class="number">1</span> - np.power(A1, <span class="number">2</span>)))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">&quot;dW1&quot;</span>: dW1,</span><br><span class="line">             <span class="string">&quot;db1&quot;</span>: db1,</span><br><span class="line">             <span class="string">&quot;dW2&quot;</span>: dW2,</span><br><span class="line">             <span class="string">&quot;db2&quot;</span>: db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>更新参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate = <span class="number">1.2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有梯度的字典</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有更新后的参数的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从字典 &quot;parameters&quot; 中获取 W1, W2, b1, b2</span></span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字典 &quot;grads&quot; 中获取梯度值 dW1, dW2, db1, db2</span></span><br><span class="line">    dW1 = grads[<span class="string">&quot;dW1&quot;</span>]</span><br><span class="line">    db1 = grads[<span class="string">&quot;db1&quot;</span>]</span><br><span class="line">    dW2 = grads[<span class="string">&quot;dW2&quot;</span>]</span><br><span class="line">    db2 = grads[<span class="string">&quot;db2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新每个参数</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2    </span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">                  <span class="string">&quot;b2&quot;</span>: b2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<ol start="7">
<li>函数整合</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_model</span>(<span class="params">X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 数据集，shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 标签，shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- 隐藏层的大小</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降循环的迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- True则每1000次迭代打印cost</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    parameters -- 模型学习到的参数，可以被用来预测</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数, 获取 W1, b1, W2, b2</span></span><br><span class="line">    <span class="comment"># Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        <span class="comment"># Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        <span class="comment"># Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        <span class="comment"># Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度下降参数更新</span></span><br><span class="line">        <span class="comment"># Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每1000次迭代打印cost</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Cost after iteration %i: %f&quot;</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<ol start="8">
<li>预测</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">parameters, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    parameters -- 包含训练好的参数的字典</span></span><br><span class="line"><span class="string">    X -- 输入数据，size (n_x, m)</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    predictions -- 模型的预测结果向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算前向传播的概率，并按概率0.5为界限进行二分分类</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = np.<span class="built_in">round</span>(A2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>
<h1>四、深层神经网络</h1>
<h2 id="4-1-前向传播">4.1 前向传播</h2>
<p><strong>（1）数学计算</strong><br>
$$z^{[1]}=W^{[1]}x+b^{[1]}$$<br>
$$a^{[1]}=g(z^{[1]})$$<br>
$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$<br>
$$a^{[2]}=g(z^{[2]})$$<br>
$$\cdots$$<br>
$$z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$$<br>
$$a^{[l]}=g(z^{[l]})$$</p>
<p><strong>（2）向量化</strong></p>
<p>$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$<br>
$$A^{[l]}=g(Z^{[l]})$$</p>
<h2 id="4-2-核对矩阵维数">4.2 核对矩阵维数</h2>
<p><strong>（1）推导过程</strong></p>
<p>对于计算过程：<br>
$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$<br>
由于$Z^{[l]}$的行数等于每一层神经元单元数$n^{l}$，列数等于样本数$m$，因此其中的$Z^{[l]}$的维数为$(n^{[l]}, m)$。<br>
$A^{[l-1]}$是由$Z^{[l-1]}$经过激活函数计算而得，因此维数与$Z^{[l-1]}$相同为$(n^{[l-1]},1)$。</p>
<p>根据矩阵计算的规则，可以知道$W^{[l]}$的维数为$(n^{[l]}, n^{[l-1]})$，$b$的维数应该与$Z^{[l]}$相同$(n^{[l]}, m)$，但是由于python的广播规则，因此程序中$b$的维数通常是$(n^{[l]}, 1)$</p>
<p><strong>（2）维数总结</strong></p>
<p>$$W^{[l]}:(n^{[l]}, n^{[l-1]})$$<br>
$$b^{[l]}:(n^{[l]}, 1)$$<br>
$$dW^{[l]}:(n^{[l]}, n^{[l-1]})$$<br>
$$db^{[l]}:(n^{[l]}, 1)$$</p>
<h2 id="4-3-参数和超参数">4.3 参数和超参数</h2>
<p>深度学习中我们用到的参数有：</p>
<ul>
<li>权值：$W^{[l]}$</li>
<li>偏置：$b^{[b]}$</li>
</ul>
<p>超参数包括：</p>
<ul>
<li>学习率：$\alpha$</li>
<li>迭代次数</li>
<li>隐藏层层数：L</li>
<li>隐藏层单元数：$n^{[l]}$</li>
<li>激活函数的选择</li>
</ul>
<p>所有的超参数都可以一定程度上决定最终得到的参数W和b。我们可以通过设定不同的超参数通过观察成本函数变化来判断当前超参数是否是最合适的，通常情况下超参数需要不断尝试检验才能找到对于当前问题最好的一个值。</p>
<h2 id="4-4-关键步骤编程实现">4.4 关键步骤编程实现</h2>
<p><strong>（1）初始化参数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    layer_dims -- 包含每一层维度的列表</span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    parameters -- 包含 &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot; 的字典</span></span><br><span class="line"><span class="string">       Wl -- 权值矩阵，shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">       bl -- 偏置向量，shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)            <span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="number">1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], layer_dims[l-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p><strong>（2）前向传播线性激活模块</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    A_prev -- 上一层激活后的结果 (或输入数据)，shape(上一层单元数, 样本数)</span></span><br><span class="line"><span class="string">    W -- 权值矩阵，shape (当前层单元数, 上一层单元数)</span></span><br><span class="line"><span class="string">    b -- 偏置向量，shape (当前层单元数, 1)</span></span><br><span class="line"><span class="string">    activation -- 本层使用的激活函数, 字符串格式: &quot;sigmoid&quot; or &quot;relu&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    A -- 激活函数后的输出</span></span><br><span class="line"><span class="string">    cache -- 包含 &quot;linear_cache&quot; and &quot;activation_cache&quot; 的字典，便于反向传播的计算</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        <span class="comment"># Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        <span class="comment"># Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z = np.dot(W, A_prev) + b</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<p><strong>（3）L层神经网络</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_forward</span>(<span class="params">X, parameters</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 输入数据，shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- 初始化参数，initialize_parameters_deep()的输出</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    AL -- 激活函数的输出</span></span><br><span class="line"><span class="string">    caches -- 包括</span></span><br><span class="line"><span class="string">                每层linear_relu_forward()的cache (一共L-1个, 索引从0到L-2)</span></span><br><span class="line"><span class="string">                linear_sigmoid_forward()的cache (只有一个, 索引为L-1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>                  <span class="comment"># 神经网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [LINEAR -&gt; RELU]*(L-1). 添加 &quot;cache&quot; 到 &quot;caches&quot; 列表.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 实现 LINEAR -&gt; SIGMOID. 添加 &quot;cache&quot; 到 &quot;caches&quot; 列表.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure>
<p><strong>（4）成本函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">AL, Y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    AL -- 与输入的标签匹配的预测值向量, shape(1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真值标签向量 (例如: 不是猫为0，猫为1), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    cost -- 成本函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据al, y计算成本函数</span></span><br><span class="line">    cost = -<span class="number">1</span> / m * np.<span class="built_in">sum</span>(Y * np.log(AL) + (<span class="number">1</span>-Y) * np.log(<span class="number">1</span>-AL),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 保证成本函数的向量是我们想要的，例如[[17]]变成17</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<p><strong>（5）线性部分反向传播</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_backward</span>(<span class="params">dZ, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    dZ -- 第L层的线性输出的梯度</span></span><br><span class="line"><span class="string">    cache -- (A_prev, W, b)组成的元组，来自当前层的前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层激活函数后的梯度</span></span><br><span class="line"><span class="string">    dW -- W的梯度</span></span><br><span class="line"><span class="string">    db -- b的梯度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.<span class="built_in">sum</span>(dZ,axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<p><strong>（6）线性激活部分反向传播</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    dA -- 当前层激活后的梯度</span></span><br><span class="line"><span class="string">    cache -- (linear_cache, activation_cache)组成的元组，为了反向传播计算更快</span></span><br><span class="line"><span class="string">    activation -- 当前层所用的激活函数, 以字符串格式存储: &quot;sigmoid&quot; or &quot;relu&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    dA_prev -- 上一层激活后的梯度</span></span><br><span class="line"><span class="string">    dW -- W的梯度</span></span><br><span class="line"><span class="string">    db -- b的梯度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<p><strong>（7）线性模型反向传播</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_backward</span>(<span class="params">AL, Y, caches</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    AL -- 概率向量, 前向传播的输出(L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- 数据集真值向量</span></span><br><span class="line"><span class="string">    caches -- 列表包括</span></span><br><span class="line"><span class="string">		linear_activation_forward() 函数的cache (是caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">		linear_activation_forward() 函数的cache (是caches[L-1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    grads -- 梯度的字典</span></span><br><span class="line"><span class="string">		grads[&quot;dA&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">		grads[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">		grads[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches) <span class="comment"># 层数</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># 此行后，Y与YL相同</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化反向传播</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    <span class="comment"># 第L层梯度 (SIGMOID -&gt; LINEAR). 输入: &quot;AL, Y, caches&quot;. 输出: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span></span><br><span class="line">    current_cache = caches[L-<span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L - <span class="number">1</span>)):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># l层梯度 (RELU -&gt; LINEAR)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入: &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. 输出: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)]</span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l+<span class="number">2</span>)], current_cache, activation = <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p><strong>（8）参数更新</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的python字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有梯度的字典, L_model_backward 函数的输出</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回值:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有更新后参数的字典</span></span><br><span class="line"><span class="string">		parameters[&quot;W&quot; + str(l)] = parameters[&quot;W&quot; + str(l)] - learning_rate * grads[&quot;dW&quot; + str(l + 1)]</span></span><br><span class="line"><span class="string">		parameters[&quot;b&quot; + str(l)] = parameters[&quot;b&quot; + str(l)] - learning_rate * grads[&quot;db&quot; + str(l + 1)] </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数更新</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(l + <span class="number">1</span>)]    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>【深度学习笔记01】神经网络与深度学习</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.mahaofei.com/post/deeplearning-note1.html">https://www.mahaofei.com/post/deeplearning-note1.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>马浩飞</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2022-05-20</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2022-05-20</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a></div><div class="post_share"><div class="social-share" data-image="https://img.mahaofei.com/img/20220426114017.png" data-sites="qq,wechat,weibo,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/deeplearning-basic.html"><img class="prev-cover" src="https://pytorch.org/assets/images/pytorch-logo.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">如何理解卷积神经网络中的通道channel</div></div></a></div><div class="next-post pull-right"><a href="/post/maskrcnn.html"><img class="next-cover" src="https://img.mahaofei.com/img/20220515154938.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">经典实例分割模型Mask RCNN</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><a href="https://www.mahaofei.com"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="author-info__name">马浩飞</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-solid fa-graduation-cap"></i><span>学术主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HaofeiMa" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mail@mahaofei.com" target="_blank" title="E-Mail"><i class="fa fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">新增了<a target="_blank" rel="noopener" href="https://academic.mahaofei.com/">学术主页</a>！<br>有任何问题欢迎留言评论或邮件联系。<br>E-mail：<a href="mailto:blog@mahaofei.com" style="text-decoration:underline;">blog@mahaofei.com</a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">一、深度学习概论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">1.1 神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.2 监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E5%85%B3%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.3 关于深度学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">二、神经网络编程基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="toc-text">2.1 符号定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94logistic%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95"><span class="toc-text">2.2 二元分类问题——logistic回归方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">2.3 梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">2.4 计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-text">2.5 向量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-%E5%85%B3%E4%BA%8Enumpy%E4%B8%8E%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-text">2.6 关于numpy与常用函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-%E6%80%BB%E7%BB%93"><span class="toc-text">2.7 总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">三、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-text">3.1 神经网络的表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-text">3.2 神经网络的计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">3.3 激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.4 神经网络的梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">3.5 参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.6 代码实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">四、深层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">4.1 前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%A0%B8%E5%AF%B9%E7%9F%A9%E9%98%B5%E7%BB%B4%E6%95%B0"><span class="toc-text">4.2 核对矩阵维数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">4.3 参数和超参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%85%B3%E9%94%AE%E6%AD%A5%E9%AA%A4%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.4 关键步骤编程实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/linemod.html" title="【6D位姿估计算法】linemod的介绍与代码测试"><img src="https://img.mahaofei.com/img/mesh%E7%BD%91%E6%A0%BC%E6%A8%A1%E5%9E%8B.png" alt="【6D位姿估计算法】linemod的介绍与代码测试"></a><div class="content"><a class="title" href="/post/linemod.html" title="【6D位姿估计算法】linemod的介绍与代码测试">【6D位姿估计算法】linemod的介绍与代码测试</a><time datetime="2022-08-13" title="发表于 2022-08-13">2022-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/convolution.html" title="卷积是如何计算的"><img src="https://pic2.zhimg.com/v2-5efbada1d4bb599e6195ce819aa7ebd9_b.webp" alt="卷积是如何计算的"></a><div class="content"><a class="title" href="/post/convolution.html" title="卷积是如何计算的">卷积是如何计算的</a><time datetime="2022-05-16" title="发表于 2022-05-16">2022-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/deeplearning-basic.html" title="如何理解卷积神经网络中的通道channel"><img src="https://pytorch.org/assets/images/pytorch-logo.png" alt="如何理解卷积神经网络中的通道channel"></a><div class="content"><a class="title" href="/post/deeplearning-basic.html" title="如何理解卷积神经网络中的通道channel">如何理解卷积神经网络中的通道channel</a><time datetime="2022-05-17" title="发表于 2022-05-17">2022-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/channel.html" title="如何理解卷积神经网络中的通道channel"><img src="https://img.mahaofei.com/img/20220624185923.png" alt="如何理解卷积神经网络中的通道channel"></a><div class="content"><a class="title" href="/post/channel.html" title="如何理解卷积神经网络中的通道channel">如何理解卷积神经网络中的通道channel</a><time datetime="2022-05-16" title="发表于 2022-05-16">2022-05-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/tensorboard.html" title="TensorBoard的使用丨深度学习曲线生成"><img src="https://img.mahaofei.com/img/20230323221410.png" alt="TensorBoard的使用丨深度学习曲线生成"></a><div class="content"><a class="title" href="/post/tensorboard.html" title="TensorBoard的使用丨深度学习曲线生成">TensorBoard的使用丨深度学习曲线生成</a><time datetime="2023-03-23" title="发表于 2023-03-23">2023-03-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/deeplearning-note2.html" title="【深度学习笔记02】数据操作的实现与线性代数基础"><img src="https://img.mahaofei.com/img/20220719115313.png" alt="【深度学习笔记02】数据操作的实现与线性代数基础"></a><div class="content"><a class="title" href="/post/deeplearning-note2.html" title="【深度学习笔记02】数据操作的实现与线性代数基础">【深度学习笔记02】数据操作的实现与线性代数基础</a><time datetime="2022-07-19" title="发表于 2022-07-19">2022-07-19</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img.mahaofei.com/img/GoodnightCopenhagen.png')"><div id="footer-wrap"><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Hosted-Github-brightgreen?style=flat&logo=GitHub"title="本站项目由Gtihub托管"></a><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?logoColor=white&style=flat&logo=buefy"title="主题采用butterfly"></a><a style="margin-inline:5px"target="_blank"href="https://aliyun.com/product/cdn"><img src="https://img.shields.io/badge/DNS-Cloudflare-orange?style=flat&logo=Cloudflare"title="本站使用Cloudflare网络服务"></a><a style="margin-inline:5px"target="_blank"href="https://beian.miit.gov.cn/"><img src="https://img.shields.io/badge/%E6%B4%A5ICP%E5%A4%87-2021000769%E5%8F%B7--3-red?logo=data:image/png;base64,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"title="备案号:津ICP备2021000769号-3"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><i class="fas fa-adjust"><use id="modeicon" xlink:href="#icon-moon"></use></i></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Algolia</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.vemoji)'))
      }
    }, "https://cdn.jsdelivr.net/gh/zhheo/twikoo@dev/dist/twikoo.all.min.js"))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      document.getElementById('twikoo-count').innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.min.js"></script><script src="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/lib/index.js"></script><script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script><script src="/js/custom/categoryBar.js"></script><script src="/js/custom/cardHistory.js"></script><script src="/js/custom/light_dark.js"></script><script src="/js/custom/forbidCopy.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"Jfmc1FSFWs09EC8r",ck:"Jfmc1FSFWs09EC8r"})</script><script src="https://sdk.51.la/perf/js-sdk-perf.min.js" crossorigin="anonymous"></script><script>new LingQue.Monitor().init({id:"JfmcYvlVsVAZlVkS"});</script><script defer src="https://cloud.umami.is/script.js" data-website-id="86b466f8-e8bc-415b-954e-289b3d0110fb"></script><script src="/js/custom/custom.js"></script><script src="/js/custom/nav.js"></script><script id="canvas_nest" defer="defer" color="66,66,66" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>