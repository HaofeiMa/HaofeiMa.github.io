<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【6D位姿估计算法】Gen6D算法 | 马浩飞丨博客</title><meta name="keywords" content="深度学习,实验,视觉,位姿估计,Gen6D"><meta name="author" content="马浩飞"><meta name="copyright" content="马浩飞"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="算法复现">
<meta property="og:type" content="article">
<meta property="og:title" content="【6D位姿估计算法】Gen6D算法">
<meta property="og:url" content="https://www.mahaofei.com/post/gen6d.html">
<meta property="og:site_name" content="马浩飞丨博客">
<meta property="og:description" content="算法复现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img.mahaofei.com/img/20230327215520.png">
<meta property="article:published_time" content="2023-03-29T13:22:39.000Z">
<meta property="article:modified_time" content="2023-03-29T13:22:39.000Z">
<meta property="article:author" content="马浩飞">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="实验">
<meta property="article:tag" content="视觉">
<meta property="article:tag" content="位姿估计">
<meta property="article:tag" content="Gen6D">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img.mahaofei.com/img/20230327215520.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.mahaofei.com/post/gen6d"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-bB89NudWgv"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?439a0d0abeb31dd8f338efd8266c999b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ABY1KMOQQM","apiKey":"d3f3a4fbb355106e6bf265cf8da1863b","indexName":"hexo","hits":{"per_page":4},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【6D位姿估计算法】Gen6D算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-29 21:22:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://unpkg.com/swiper/swiper-bundle.min.css"><link rel="stylesheet" href="/css/custom/MainColor.css"><link rel="stylesheet" href="/css/custom/categoryBar.css"><link rel="stylesheet" href="/css/custom/404.css"><link rel="stylesheet" href="/css/custom/cardHistory.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><link rel="stylesheet" href="/css/custom/custom.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="马浩飞丨博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img.mahaofei.com/img/20230327215520.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">马浩飞丨博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-history"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-solid fa-envelope-open-text"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 本站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/"><i class="fa-fw fa-solid fa-blog"></i><span> 个人博客</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-fw fa-solid fa-graduation-cap"></i><span> 学术主页</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://nav.mahaofei.com/"><i class="fa-fw fas fa-compass"></i><span> 导航网站</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【6D位姿估计算法】Gen6D算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-29T13:22:39.000Z" title="发表于 2023-03-29 21:22:39">2023-03-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-29T13:22:39.000Z" title="更新于 2023-03-29 21:22:39">2023-03-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E7%89%A9%E4%BD%93%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/">物体位姿估计</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【6D位姿估计算法】Gen6D算法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/gen6d.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><blockquote>
<p><strong>标题</strong>：Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images<br><strong>作者团队</strong>：The University of Hong Kong<br><strong>期刊会议</strong>：ECCV<br><strong>时间</strong>：2022<br><strong>代码</strong>：<a target="_blank" rel="noopener" href="https://github.com/liuyuan-pal/Gen6D">https://github.com/liuyuan-pal/Gen6D</a></p>
</blockquote>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-目标问题"><a href="#1-1-目标问题" class="headerlink" title="1.1 目标问题"></a>1.1 目标问题</h3><p>现有的位姿估计算法要么需要高质量的物体模型，要么需要提供额外的深度图或物体掩码图，这对于位姿估计的实际应用有很大的限制。本文提出的方法只需要一些物体的姿态图像，就能够在任意环境中预测物体位姿。</p>
<p>作者认为一个位姿估计器应该具有以下特点：</p>
<ul>
<li>通用性：可以应用于任意物体，而无需对对象或类别进行训练</li>
<li>无模型：用于一个未见过的物体时，只需要一些已知姿态的参考图像来定义物体参考坐标系即可</li>
<li>输入简单：仅输入RGB图像来估计位姿，而不需要深度图或物体掩码图</li>
</ul>
<p><strong>（1）如何设计视角选择器，从参考图像中找到与查询图像视角最接近的</strong></p>
<p>本文使用神经网络对查询图像和参考图像进行逐像素比较，产生相似性得分，并选择具有最高相似性得分的参考图像。并添加了全局归一化层和自注意层来共享不同参考图像之间的相似性信息，为选择最相似的参考图像提供了上下文信息。</p>
<p><strong>（2）实现没有模型的姿态优化</strong></p>
<p>本文提出了一种新的基于三维空间的姿态优化方法，给定一个查询图像和一个输入姿态，找到几个接近输入姿态的参考图像，将这些参考图像投影回3D空间中，构建特征空间，通过3D的CNN将构建的特征空间与查询图像的特征相匹配，来优化姿态。</p>
<h3 id="1-2-现有工作"><a href="#1-2-现有工作" class="headerlink" title="1.2 现有工作"></a>1.2 现有工作</h3><p>现有位姿估计方法大都是基于特定实例的，不能推广到未见过的物体，通常都需要根据物体3D模型来渲染大量图像进行训练。有一些方法可以推广到类别级，也不需要对象的模型，但仍然无法预测没见过的类别的物体。</p>
<h2 id="2-实现方法"><a href="#2-实现方法" class="headerlink" title="2. 实现方法"></a>2. 实现方法</h2><p><strong>数据规范化</strong>：对于每个物体，通过对参考图像中的点进行三角测量等方法估计物体的大致大小，然后对物体坐标系进行归一化，使物体中心位于原点，大小为1，此时物体位于原点的单位球体内。</p>
<p>Gen6D包括一个物体检测器，一个视角选择器，一个姿态优化器。</p>
<p><img src="https://img.mahaofei.com/img/20230403203020.png" alt="image.png"></p>
<p>物体检测其首先利用查询图像和参考图像来检测物体所在区域。然后视角选择器将查询图像于参考图像相匹配，产生粗略的初始姿态。最后由姿态优化器进一步细化以得到精确的对象姿态。</p>
<h3 id="2-1-物体检测"><a href="#2-1-物体检测" class="headerlink" title="2.1 物体检测"></a>2.1 物体检测</h3><p>将检测问题分解成两部分</p>
<ol>
<li>找到对象中心的2D投影点q</li>
<li>估计包围单位球体的正方形边界框。</li>
</ol>
<p><img src="https://img.mahaofei.com/img/20230403204751.png" alt="image.png"></p>
<p>物体中心的深度可以使用$d=2f/S_q$求得，其中2是单位球体的直径，f是虚拟焦距（将主点设为投影点q），$S_q$是边界框边长。这就是物体的初始平移。</p>
<blockquote>
<p>问题：这里将物体归一化之后求出的深度d还是真实深度吗？虚拟焦距又是如何确定的？</p>
</blockquote>
<p>检测器使用了VGG网络提取参考图像和查询图像的特征图，然后将所有参考图像的特征图作为卷积核与查询图像的特征图卷积，得到分数图。考虑尺度差异，设置再多个预定义尺度上进行卷积，最后得到热力图和比例图。选择热力图上的最大值位置作为对象中心2D投影，使用比例图上相同比例的比例作为边界框的大小$S_q=S_r*s$。</p>
<blockquote>
<p>问题：这里将所有参考图像的特征图都进行卷积，那么参考图像上物体特征和背景特征是如何区分的？</p>
</blockquote>
<h3 id="2-2-视角选择"><a href="#2-2-视角选择" class="headerlink" title="2.2 视角选择"></a>2.2 视角选择</h3><p>将查询图像与每个参考图像比较，计算相似性得分。计算每个参考图像和查询图像的元素乘积，获得得分图，并计算相似性参数。</p>
<p><img src="https://img.mahaofei.com/img/20230404192903.png" alt="image.png"></p>
<p><strong>（1）平面内旋转</strong><br>为了考虑平面内旋转，本文将参考图像旋转Na个预定义角度，查询时使用所有旋转版本进行逐元素乘积。</p>
<p><strong>（2）全局归一化</strong><br>使用参考图像的所有特征图计算的均值和方差，对相似度网络生成的特征图进行归一化。这样做可以用特征图的分布来编码上下文相似性，并放大不同图像之间的相似性差异。</p>
<p><strong>（3）参考视角变换</strong><br>在所有参考图像的相似性特征向量上应用变换，包括它们的视角、注意力层。这样的变换器使得特征向量相互通信以编码上下文信息，有助于确定最相似的参考图像。</p>
<h3 id="2-3-姿态优化"><a href="#2-3-姿态优化" class="headerlink" title="2.3 姿态优化"></a>2.3 姿态优化</h3><p>经过上面两个步骤，我们已经有了粗略的物体位姿。本步骤对位姿进行优化。</p>
<p>选择接近输入姿态的6个参考图像，通过CNN提取特征图，然后将特征图投影到3D空间中，并计算特征的均值和方差作为空间顶点的特征。<br>对于查询图像，使用同样的CNN提取特征图，将特征图投影到3D空间中，并将查询特征与参考图像特征的均值和方差连接起来。</p>
<p>最后在空间特征上使用3DCNN预测残差来更新输入姿态。</p>
<p><img src="https://img.mahaofei.com/img/20230404195340.png" alt="image.png"></p>
<h1 id="3-实验分析"><a href="#3-实验分析" class="headerlink" title="3. 实验分析"></a>3. 实验分析</h1><h1 id="二、算法复现"><a href="#二、算法复现" class="headerlink" title="二、算法复现"></a>二、算法复现</h1><h2 id="2-1-环境搭建"><a href="#2-1-环境搭建" class="headerlink" title="2.1 环境搭建"></a>2.1 环境搭建</h2><h3 id="2-1-1-Python环境"><a href="#2-1-1-Python环境" class="headerlink" title="2.1.1 Python环境"></a>2.1.1 Python环境</h3><p>创建[[02_Anaconda的基本使用与在Pycharm中调用|Anaconda虚拟环境]]</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n gen6d python=3.7</span><br><span class="line">conda activate gen6d</span><br></pre></td></tr></table></figure>
<p>安装pytorch环境</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 -c pytorch</span><br></pre></td></tr></table></figure>
<p>安装依赖，打开<code>requirements.txt</code>，删除其中的pytorch, torchvision, cudatoolkit</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-自制数据集工具"><a href="#2-1-2-自制数据集工具" class="headerlink" title="2.1.2 自制数据集工具"></a>2.1.2 自制数据集工具</h3><p><strong>（1）COLMAP</strong></p>
<p>参考官网教程：<a target="_blank" rel="noopener" href="https://colmap.github.io/install.html">https://colmap.github.io/install.html</a></p>
<p>安装依赖库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">    git \</span><br><span class="line">    cmake \</span><br><span class="line">    build-essential \</span><br><span class="line">    libboost-program-options-dev \</span><br><span class="line">    libboost-filesystem-dev \</span><br><span class="line">    libboost-graph-dev \</span><br><span class="line">    libboost-regex-dev \</span><br><span class="line">    libboost-system-dev \</span><br><span class="line">    libboost-test-dev \</span><br><span class="line">    libeigen3-dev \</span><br><span class="line">    libsuitesparse-dev \</span><br><span class="line">    libfreeimage-dev \</span><br><span class="line">    libgoogle-glog-dev \</span><br><span class="line">    libgflags-dev \</span><br><span class="line">    libglew-dev \</span><br><span class="line">    qtbase5-dev \</span><br><span class="line">    libqt5opengl5-dev \</span><br><span class="line">    libcgal-dev \</span><br><span class="line">    libcgal-qt5-dev\</span><br><span class="line">    libceres-dev\</span><br><span class="line">    ninja-build\</span><br><span class="line">    libmetis-dev</span><br></pre></td></tr></table></figure>
<p>下载COLMAP源代码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/colmap/colmap</span><br><span class="line">cd colmap</span><br></pre></td></tr></table></figure>
<p>修改<code>CMakeLists.txt</code>文件，添加下面的内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_CUDA_ARCHITECTURES 86)</span><br></pre></td></tr></table></figure>
<p>开始编译、安装（注意一定要退出conda环境再编译安装）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -GNinja</span><br><span class="line">ninja</span><br><span class="line">sudo ninja install</span><br></pre></td></tr></table></figure>
<p><strong>（2）CloudCompare</strong></p>
<p><strong>方式1：snap（推荐）</strong></p>
<p>安装snap</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install snap</span><br></pre></td></tr></table></figure>
<p>安装cloudcompare</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">snap install cloudcompare</span><br></pre></td></tr></table></figure>
<p>启动cloudcompare</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure>
<p><strong>方式2：Flatpak</strong></p>
<p>安装Flatpak</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install flatpak</span><br></pre></td></tr></table></figure>
<p>安装Software Flatpak plugin</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install gnome-software-plugin-flatpak</span><br></pre></td></tr></table></figure>
<p>添加Flathub repository</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo</span><br></pre></td></tr></table></figure>
<p>安装CloudCompare</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak install flathub org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure>
<p>运行CloudCompare</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure>
<p><strong>（3）安装ffmpeg</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ffmpeg</span><br></pre></td></tr></table></figure>
<h2 id="2-2-数据集准备"><a href="#2-2-数据集准备" class="headerlink" title="2.2 数据集准备"></a>2.2 数据集准备</h2><h3 id="2-2-1-官方数据集"><a href="#2-2-1-官方数据集" class="headerlink" title="2.2.1 官方数据集"></a>2.2.1 官方数据集</h3><p><strong>（1）下载数据集</strong></p>
<p>从<a target="_blank" rel="noopener" href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">原作者给出的链接</a>中下载预训练模型，GenMOP数据集和processed LINEMOD数据集。</p>
<p><strong>（2）组织数据集</strong></p>
<p>将下载的文件按照下面的格式进行整理。</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- GenMOP</span><br><span class="line">        |-- chair </span><br><span class="line">            ...</span><br><span class="line">    |-- LINEMOD</span><br><span class="line">        |-- cat </span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>
<h3 id="2-2-2-自制数据集"><a href="#2-2-2-自制数据集" class="headerlink" title="2.2.2 自制数据集"></a>2.2.2 自制数据集</h3><p><strong>（1）视频录制</strong></p>
<p>使用手机录制目标物体的参考视频和测试视频。注意：参考视频需要满足以下条件</p>
<ul>
<li>参考视频中对象是静态的</li>
<li>参考视频中背景尽可能纹理丰富且平整，摄像角度要尽可能覆盖每个角度，以便COLMAP恢复相机姿态</li>
</ul>
<p><strong>（2）组织文件</strong></p>
<p>将视频按照下面的路径进行组织</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- video</span><br><span class="line">           |-- mouse-ref.mp4</span><br><span class="line">           |-- mouse-test.mp4</span><br></pre></td></tr></table></figure>
<p><strong>（3）将参考视频拆分为图像</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 每10帧保存一张图像，最大图像边长为960</span></span></span><br><span class="line">python prepare.py --action video2image \</span><br><span class="line">                  --input data/custom/video/ref/realsensebox-ref.mp4 \</span><br><span class="line">                  --output data/custom/realsensebox/images \</span><br><span class="line">                  --frame_inter 10 \</span><br><span class="line">                  --image_size 960 \</span><br><span class="line">                  --transpose</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 或者</span></span></span><br><span class="line">python prepare.py --action video2image --input data/custom/video/ammeter-ref.mp4 --output data/custom/ammeter/images --frame_inter 10 --image_size 960 --transpose</span><br></pre></td></tr></table></figure>
<p>拆分后的视频保存在<code>data/custom/coffeebox/images</code>中。</p>
<p><strong>（4）运行COLMAP SfM恢复相机姿态</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action sfm --database_name custom/realsensebox --colmap /usr/local/bin/colmap</span><br></pre></td></tr></table></figure>
<p>注：<code>&lt;path-to-your-colmap-exe&gt;</code>可以通过命令<code>which colmap</code>来查找，一般ubuntu路径为<code>/usr/local/bin/colmap</code>，windows路径为<code>E:/Programming/COLMAP-3.8-windows-cuda/COLMAP.bat</code></p>
<p><strong>（5）手动处理点云</strong></p>
<p>通过裁减对象点云来手动确定对象所在区域。例如使用<a target="_blank" rel="noopener" href="https://www.cloudcompare.org/">CloudCompare</a>来可视化处理COLMAP重建的点云，重建的点云位于<code>data/custom/mouse/colmap/pointcloud.ply</code>中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flatpak run org.cloudcompare.CloudCompare</span><br></pre></td></tr></table></figure>
<p><img src="https://img.mahaofei.com/img/20230327215520.png" alt=""></p>
<p>导出裁剪后的点云为<code>data/custom/mouse/object_point_cloud.ply</code>。</p>
<p><img src="https://img.mahaofei.com/img/20230327220042.png" alt=""></p>
<p><strong>（6）手动确定对象的X轴正方向和Z轴正方向</strong></p>
<p><img src="https://img.mahaofei.com/img/20230327220221.png" alt=""></p>
<p><img src="https://img.mahaofei.com/img/20230327220225.png" alt=""></p>
<p>编辑一个<code>data/custom/mouse/meta_info.txt</code>文件来保存你的X+和Z+信息，例如</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.297052 0.350839 -0.000593</span><br><span class="line">0.973488 0.054352 -0.222188</span><br></pre></td></tr></table></figure>
<p><strong>（7）确保您具有以下文件，这些文件由上述步骤生成</strong></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">       |-- mouse</span><br><span class="line">           |-- object_point_cloud.ply  ## object point cloud</span><br><span class="line">           |-- meta_info.txt           ## meta information about z+/x+ directions</span><br><span class="line">           |-- images                  ## images</span><br><span class="line">           |-- colmap                  ## colmap project</span><br></pre></td></tr></table></figure>
<p><strong>（8）从处理后的参考图像中预测姿势</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python predict.py --cfg configs/gen6d_pretrain.yaml \</span><br><span class="line">                  --database custom/realsensebox \</span><br><span class="line">                  --video data/custom/video/test/realsensebox-test.mp4 \</span><br><span class="line">                  --resolution 1280 \</span><br><span class="line">                  --transpose \</span><br><span class="line">                  --output data/custom/video/test \</span><br><span class="line">                  --ffmpeg &lt;path-to-ffmpeg-exe&gt;</span><br></pre></td></tr></table></figure>
<h2 id="2-3-训练与评估"><a href="#2-3-训练与评估" class="headerlink" title="2.3 训练与评估"></a>2.3 训练与评估</h2><p>在<a target="_blank" rel="noopener" href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EkWESLayIVdEov4YlVrRShQBkOVTJwgK0bjF7chFg2GrBg?e=Y8UpXu">此处</a>下载处理后的 co3d 数据 (co3d.tar.gz)、Google 扫描对象数据 (google_scanned_objects.tar.gz) 和 ShapeNet 渲染图 (shapenet.tar.gz)，预训练模型（gen6d_pretrain.tar.gz）。</p>
<p>下载 <a target="_blank" rel="noopener" href="https://cocodataset.org/#download">COCO 2017 Train images</a></p>
<p>将文件按照下面的形式组织</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Gen6D</span><br><span class="line">|-- data</span><br><span class="line">    |-- custom</span><br><span class="line">        |-- coffebox </span><br><span class="line">            ...</span><br><span class="line">    |-- model</span><br><span class="line">        |-- detector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- selector_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">        |-- refiner_pretrain</span><br><span class="line">            |-- model_best.pth</span><br><span class="line">    |-- shapenet</span><br><span class="line">        |-- shapenet_cache</span><br><span class="line">        |-- shapenet_render</span><br><span class="line">        |-- shapenet_render_v1.pkl</span><br><span class="line">    |-- co3d_256_512</span><br><span class="line">        |-- apple</span><br><span class="line">            ...</span><br><span class="line">    |-- google_scanned_objects</span><br><span class="line">        |-- 06K3jXvzqIM</span><br><span class="line">            ...</span><br><span class="line">    |-- coco</span><br><span class="line">        |-- train2017</span><br></pre></td></tr></table></figure>
<h3 id="2-3-1-训练detector"><a href="#2-3-1-训练detector" class="headerlink" title="2.3.1 训练detector"></a>2.3.1 训练detector</h3><p>修改<code>train_meta_info.py</code>的第86行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;genmop_train&#x27;</span>: [<span class="string">f&#x27;genmop/<span class="subst">&#123;name&#125;</span>-test&#x27;</span> <span class="keyword">for</span> name <span class="keyword">in</span> [<span class="string">&#x27;ammeter&#x27;</span>, <span class="string">&#x27;coffeebox&#x27;</span>, <span class="string">&#x27;realsensebox&#x27;</span>]],</span><br></pre></td></tr></table></figure>
<p>修改<code>database.py</code>的第109行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GenMOP_ROOT = <span class="string">&#x27;data/custom&#x27;</span></span><br><span class="line"></span><br><span class="line">genmop_meta_info=&#123;</span><br><span class="line">    <span class="string">&#x27;ammeter&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0222805</span>, -<span class="number">0.409031</span>, <span class="number">0.912248</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.401556</span>, <span class="number">0.773825</span>, <span class="number">0.340199</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;coffeebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.0718405</span>, -<span class="number">0.471545</span>, <span class="number">0.878911</span>]), <span class="string">&#x27;forward&#x27;</span>: np.asarray([<span class="number">0.582604</span>, -<span class="number">0.490501</span>, -<span class="number">0.219265</span>],np.float32)&#125;,</span><br><span class="line">    <span class="string">&#x27;realsensebox&#x27;</span>: &#123;<span class="string">&#x27;gravity&#x27;</span>: np.asarray([<span class="number">0.103463</span>, -<span class="number">0.521284</span>, <span class="number">0.847088</span>],np.float32), <span class="string">&#x27;forward&#x27;</span>: np.asarray([-<span class="number">1.690831</span>, <span class="number">0.688506</span>, <span class="number">0.590004</span>],np.float32)&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改<code>database.py</code>的第212行，修改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras, images, points3d = read_model(<span class="string">f&#x27;<span class="subst">&#123;GenMOP_ROOT&#125;</span>/<span class="subst">&#123;seq_name&#125;</span>/colmap/sparse/0&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>开始训练</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/detector/detector_train.yaml</span><br></pre></td></tr></table></figure>
<h3 id="2-3-2-训练selector"><a href="#2-3-2-训练selector" class="headerlink" title="2.3.2 训练selector"></a>2.3.2 训练selector</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/selector/selector_train.yaml</span><br></pre></td></tr></table></figure>
<h3 id="2-3-3-训练refiner"><a href="#2-3-3-训练refiner" class="headerlink" title="2.3.3 训练refiner"></a>2.3.3 训练refiner</h3><p>为refiner训练进行数据准备</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database linemod/cat \</span><br><span class="line">                  --que_split linemod_val \</span><br><span class="line">                  --ref_database linemod/cat \</span><br><span class="line">                  --ref_split linemod_val</span><br><span class="line"></span><br><span class="line">python prepare.py --action gen_val_set \</span><br><span class="line">                  --estimator_cfg configs/gen6d_train.yaml \</span><br><span class="line">                  --que_database genmop/tformer-test \</span><br><span class="line">                  --que_split all \</span><br><span class="line">                  --ref_database genmop/tformer-ref \</span><br><span class="line">                  --ref_split all </span><br></pre></td></tr></table></figure>
<p>该命令会在<code>data/val</code>生成信息，该信息会被用于生成refiner的有效数据</p>
<p>训练refiner</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_model.py --cfg configs/refiner/refiner_train.yaml</span><br></pre></td></tr></table></figure>
<h3 id="2-3-4-评估所有组件"><a href="#2-3-4-评估所有组件" class="headerlink" title="2.3.4 评估所有组件"></a>2.3.4 评估所有组件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object TFormer from the GenMOP dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name genmop/tformer</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Evaluate on the object <span class="built_in">cat</span> from the LINEMOD dataset</span></span><br><span class="line">python eval.py --cfg configs/gen6d_train.yaml --object_name linemod/cat</span><br></pre></td></tr></table></figure>
<h1 id="三、现存问题"><a href="#三、现存问题" class="headerlink" title="三、现存问题"></a>三、现存问题</h1><p><strong>优点</strong></p>
<ol>
<li>只需要对给定物体录制1-2分钟的视频，使用程序1-2小时<strong>添加数据集</strong>，即可实现新物体的位姿估计，不需要再训练网络</li>
<li><strong>精度</strong>还可以</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>对于<strong>方形凸形物体识别较好，对于物体内部存在中空区域</strong>，例如圆环等物体识别效果较差</li>
<li>由于<strong>参考视频要求物体静止，因此无法录到物体底面的特征</strong>，对于物体底面识别效果较差（可考虑物体正反放置录制两次，对于同一个物体使用两个参考视频进行预测，选择置信度高的位姿）</li>
<li>当进行识别时，如果<strong>图像中不存在物体也会生成一个估计位姿</strong>（可以考虑根据置信度判断输出，或者在位姿估计前使用yolo等算法预判断物体位置）</li>
<li>当存在<strong>遮挡时位姿估计效果较差</strong>，可能会出现只框处未被遮挡的部分，或者在遮挡物体上强行进行位姿估计。</li>
<li>当要同时识别的物体很多时，对于显卡显存要求比较大，而且计算会很慢，服务器1.5s/it。如果每次只对某个特定物体进行识别，速度还可以。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>【6D位姿估计算法】Gen6D算法</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.mahaofei.com/post/gen6d.html">https://www.mahaofei.com/post/gen6d.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>马浩飞</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-03-29</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2023-03-29</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E5%AE%9E%E9%AA%8C/">实验</a><a class="post-meta__tags" href="/tags/%E8%A7%86%E8%A7%89/">视觉</a><a class="post-meta__tags" href="/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/">位姿估计</a><a class="post-meta__tags" href="/tags/Gen6D/">Gen6D</a></div><div class="post_share"><div class="social-share" data-image="https://img.mahaofei.com/img/20230327215520.png" data-sites="qq,wechat,weibo,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.png" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.png" alt="微信支付"/></a><div class="post-qr-code-desc">微信支付</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/gdrnpp.html"><img class="prev-cover" src="https://img.mahaofei.com/img/20230316104020.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【6D位姿估计算法】GDRNPP算法</div></div></a></div><div class="next-post pull-right"><a href="/post/contact-graspnet.html"><img class="next-cover" src="https://img.mahaofei.com/img/20230404152359.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【抓取算法】Contact GraspNet</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><a href="https://www.mahaofei.com"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></a></div><div class="author-info__name">马浩飞</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">250</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">79</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">44</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://academic.mahaofei.com/"><i class="fa-solid fa-graduation-cap"></i><span>学术主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HaofeiMa" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mail@mahaofei.com" target="_blank" title="E-Mail"><i class="fa fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">新增了<a target="_blank" rel="noopener" href="https://academic.mahaofei.com/">学术主页</a>！<br>有任何问题欢迎留言评论或邮件联系。<br>E-mail：<a href="mailto:blog@mahaofei.com" style="text-decoration:underline;">blog@mahaofei.com</a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><span class="toc-text">论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1. 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%9B%AE%E6%A0%87%E9%97%AE%E9%A2%98"><span class="toc-text">1.1 目标问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%8E%B0%E6%9C%89%E5%B7%A5%E4%BD%9C"><span class="toc-text">1.2 现有工作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95"><span class="toc-text">2. 实现方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B"><span class="toc-text">2.1 物体检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%A7%86%E8%A7%92%E9%80%89%E6%8B%A9"><span class="toc-text">2.2 视角选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%A7%BF%E6%80%81%E4%BC%98%E5%8C%96"><span class="toc-text">2.3 姿态优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90"><span class="toc-text">3. 实验分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%A4%8D%E7%8E%B0"><span class="toc-text">二、算法复现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-text">2.1 环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-Python%E7%8E%AF%E5%A2%83"><span class="toc-text">2.1.1 Python环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E8%87%AA%E5%88%B6%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B7%A5%E5%85%B7"><span class="toc-text">2.1.2 自制数据集工具</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="toc-text">2.2 数据集准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E5%AE%98%E6%96%B9%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.2.1 官方数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E8%87%AA%E5%88%B6%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.2.2 自制数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-text">2.3 训练与评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E8%AE%AD%E7%BB%83detector"><span class="toc-text">2.3.1 训练detector</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E8%AE%AD%E7%BB%83selector"><span class="toc-text">2.3.2 训练selector</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-%E8%AE%AD%E7%BB%83refiner"><span class="toc-text">2.3.3 训练refiner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-4-%E8%AF%84%E4%BC%B0%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6"><span class="toc-text">2.3.4 评估所有组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%8E%B0%E5%AD%98%E9%97%AE%E9%A2%98"><span class="toc-text">三、现存问题</span></a></li></ol></div></div><div class="card-widget card-recommend-post"><div class="item-headline"><i class="fas fa-dharmachakra"></i><span>相关推荐</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/custom-linemod.html" title="制作自己的Linemod数据集（ObjectDatasetTools）"><img src="https://img.mahaofei.com/img/20220918154609.png" alt="制作自己的Linemod数据集（ObjectDatasetTools）"></a><div class="content"><a class="title" href="/post/custom-linemod.html" title="制作自己的Linemod数据集（ObjectDatasetTools）">制作自己的Linemod数据集（ObjectDatasetTools）</a><time datetime="2022-09-18" title="发表于 2022-09-18">2022-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/densefusion-code.html" title="【6D位姿估计算法】DenseFusion代码结构"><img src="https://img.mahaofei.com/img/20230225210258.png" alt="【6D位姿估计算法】DenseFusion代码结构"></a><div class="content"><a class="title" href="/post/densefusion-code.html" title="【6D位姿估计算法】DenseFusion代码结构">【6D位姿估计算法】DenseFusion代码结构</a><time datetime="2000-01-01" title="发表于 2000-01-01">2000-01-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/gdrnpp.html" title="【6D位姿估计算法】GDRNPP算法"><img src="https://img.mahaofei.com/img/20230316104020.png" alt="【6D位姿估计算法】GDRNPP算法"></a><div class="content"><a class="title" href="/post/gdrnpp.html" title="【6D位姿估计算法】GDRNPP算法">【6D位姿估计算法】GDRNPP算法</a><time datetime="2023-03-27" title="发表于 2023-03-27">2023-03-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/contact-graspnet.html" title="【抓取算法】Contact GraspNet"><img src="https://img.mahaofei.com/img/20230404152359.png" alt="【抓取算法】Contact GraspNet"></a><div class="content"><a class="title" href="/post/contact-graspnet.html" title="【抓取算法】Contact GraspNet">【抓取算法】Contact GraspNet</a><time datetime="2023-04-07" title="发表于 2023-04-07">2023-04-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/rgb-matters.html" title="【抓取姿态估计算法】RGB Matters论文笔记与复现"><img src="https://img.mahaofei.com/img/202304231549478.png" alt="【抓取姿态估计算法】RGB Matters论文笔记与复现"></a><div class="content"><a class="title" href="/post/rgb-matters.html" title="【抓取姿态估计算法】RGB Matters论文笔记与复现">【抓取姿态估计算法】RGB Matters论文笔记与复现</a><time datetime="2023-05-04" title="发表于 2023-05-04">2023-05-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/maskrcnn.html" title="经典实例分割模型Mask RCNN"><img src="https://img.mahaofei.com/img/20220515154938.png" alt="经典实例分割模型Mask RCNN"></a><div class="content"><a class="title" href="/post/maskrcnn.html" title="经典实例分割模型Mask RCNN">经典实例分割模型Mask RCNN</a><time datetime="2022-06-11" title="发表于 2022-06-11">2022-06-11</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img.mahaofei.com/img/GoodnightCopenhagen.png')"><div id="footer-wrap"><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Hosted-Github-brightgreen?style=flat&logo=GitHub"title="本站项目由Gtihub托管"></a><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?logoColor=white&style=flat&logo=buefy"title="主题采用butterfly"></a><a style="margin-inline:5px"target="_blank"href="https://aliyun.com/product/cdn"><img src="https://img.shields.io/badge/DNS-Cloudflare-orange?style=flat&logo=Cloudflare"title="本站使用Cloudflare网络服务"></a><a style="margin-inline:5px"target="_blank"href="https://beian.miit.gov.cn/"><img src="https://img.shields.io/badge/%E6%B4%A5ICP%E5%A4%87-2021000769%E5%8F%B7--3-red?logo=data:image/png;base64,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"title="备案号:津ICP备2021000769号-3"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><i class="fas fa-adjust"><use id="modeicon" xlink:href="#icon-moon"></use></i></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Algolia</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.vemoji)'))
      }
    }, "https://cdn.jsdelivr.net/gh/zhheo/twikoo@dev/dist/twikoo.all.min.js"))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.mahaofei.com/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      document.getElementById('twikoo-count').innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.min.js"></script><script src="https://cdn.jsdelivr.net/npm/element-ui@2.15.6/lib/index.js"></script><script src="https://unpkg.com/swiper/swiper-bundle.min.js"></script><script src="/js/custom/categoryBar.js"></script><script src="/js/custom/cardHistory.js"></script><script src="/js/custom/light_dark.js"></script><script src="/js/custom/forbidCopy.js"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"Jfmc1FSFWs09EC8r",ck:"Jfmc1FSFWs09EC8r"})</script><script src="https://sdk.51.la/perf/js-sdk-perf.min.js" crossorigin="anonymous"></script><script>new LingQue.Monitor().init({id:"JfmcYvlVsVAZlVkS"});</script><script defer src="https://cloud.umami.is/script.js" data-website-id="86b466f8-e8bc-415b-954e-289b3d0110fb"></script><script src="/js/custom/custom.js"></script><script src="/js/custom/nav.js"></script><script id="canvas_nest" defer="defer" color="66,66,66" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>